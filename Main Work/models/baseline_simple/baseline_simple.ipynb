{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Baseline (Nucleotide Frequency → Linear) for 4-Component Prediction\n",
        "\n",
        "This notebook trains a very simple baseline model that converts each DNA sequence into a 5-base frequency vector (A, T, G, C, N) and applies a linear classifier to predict the 4 component probabilities. It uses the existing training utilities for a fair comparison with other models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch: 2.6.0, CUDA: False, MPS: True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "ROOT = \"/Users/jaydenthai/Dev/University Work/2025sem2/DS-Research-Project-Tumor-Expression\"\n",
        "MODELS_DIR = f\"{ROOT}/Main Work/models\"\n",
        "if MODELS_DIR not in sys.path:\n",
        "    sys.path.append(MODELS_DIR)\n",
        "\n",
        "from baseline_simple.model import BaselineLinear, BaselineFrequencyDataset\n",
        "from utils.data import load_and_prepare_data\n",
        "from utils.training import train_epoch, validate_epoch, evaluate_model\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}, MPS: {getattr(torch.backends, 'mps', None) is not None and torch.backends.mps.is_available()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Samples: 8735, targets: (8735, 4)\n",
            "Splits — train: 5590, val: 1398, test: 1747\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "sequences, targets = load_and_prepare_data(f\"{ROOT}/Main Work/Processed-Data/ProSeq_with_4component_analysis.csv\")\n",
        "print(f\"Samples: {len(sequences)}, targets: {targets.shape}\")\n",
        "\n",
        "# Stratified split by dominant component\n",
        "labels = np.argmax(targets, axis=1)\n",
        "train_seq, test_seq, train_targets, test_targets = train_test_split(\n",
        "    sequences, targets, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "train_labels = np.argmax(train_targets, axis=1)\n",
        "train_seq, val_seq, train_targets, val_targets = train_test_split(\n",
        "    train_seq, train_targets, test_size=0.2, random_state=42, stratify=train_labels\n",
        ")\n",
        "\n",
        "# Datasets/loaders\n",
        "train_ds = BaselineFrequencyDataset(train_seq, train_targets)\n",
        "val_ds = BaselineFrequencyDataset(val_seq, val_targets)\n",
        "test_ds = BaselineFrequencyDataset(test_seq, test_targets)\n",
        "\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "print(f\"Splits — train: {len(train_ds)}, val: {len(val_ds)}, test: {len(test_ds)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Params: 24\n",
            "Device: mps\n"
          ]
        }
      ],
      "source": [
        "# Build model\n",
        "hidden = 0  # set >0 for a tiny MLP\n",
        "model = BaselineLinear(input_channels=5, num_classes=4, hidden=hidden, dropout=0.1)\n",
        "\n",
        "# Device\n",
        "if getattr(torch.backends, 'mps', None) is not None and torch.backends.mps.is_available():\n",
        "    device = torch.device('mps')\n",
        "else:\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Optim\n",
        "criterion = nn.KLDivLoss(reduction='batchmean')\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
        "\n",
        "print(f\"Params: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 001/60 - train: 1.041880  val: 1.027721  lr: 1.00e-03\n",
            "Epoch 002/60 - train: 1.006747  val: 0.996548  lr: 1.00e-03\n",
            "Epoch 003/60 - train: 0.979309  val: 0.972090  lr: 1.00e-03\n",
            "Epoch 004/60 - train: 0.958266  val: 0.952933  lr: 1.00e-03\n",
            "Epoch 005/60 - train: 0.940947  val: 0.938099  lr: 1.00e-03\n",
            "Epoch 006/60 - train: 0.928366  val: 0.926598  lr: 1.00e-03\n",
            "Epoch 007/60 - train: 0.918685  val: 0.917456  lr: 1.00e-03\n",
            "Epoch 008/60 - train: 0.910383  val: 0.910585  lr: 1.00e-03\n",
            "Epoch 009/60 - train: 0.904159  val: 0.905194  lr: 1.00e-03\n",
            "Epoch 010/60 - train: 0.899666  val: 0.901111  lr: 1.00e-03\n",
            "Epoch 011/60 - train: 0.896838  val: 0.897973  lr: 1.00e-03\n",
            "Epoch 012/60 - train: 0.893753  val: 0.895555  lr: 1.00e-03\n",
            "Epoch 013/60 - train: 0.891415  val: 0.893722  lr: 1.00e-03\n",
            "Epoch 014/60 - train: 0.890027  val: 0.892237  lr: 1.00e-03\n",
            "Epoch 015/60 - train: 0.888944  val: 0.891224  lr: 1.00e-03\n",
            "Epoch 016/60 - train: 0.887550  val: 0.890396  lr: 1.00e-03\n",
            "Epoch 017/60 - train: 0.886739  val: 0.889762  lr: 1.00e-03\n",
            "Epoch 018/60 - train: 0.886810  val: 0.889297  lr: 1.00e-03\n",
            "Epoch 019/60 - train: 0.885884  val: 0.888965  lr: 1.00e-03\n",
            "Epoch 020/60 - train: 0.885405  val: 0.888679  lr: 1.00e-03\n",
            "Epoch 021/60 - train: 0.885310  val: 0.888471  lr: 1.00e-03\n",
            "Epoch 022/60 - train: 0.885685  val: 0.888315  lr: 1.00e-03\n",
            "Epoch 023/60 - train: 0.885477  val: 0.888192  lr: 1.00e-03\n",
            "Epoch 024/60 - train: 0.885515  val: 0.888111  lr: 1.00e-03\n",
            "Epoch 025/60 - train: 0.884294  val: 0.888035  lr: 1.00e-03\n",
            "Epoch 026/60 - train: 0.885669  val: 0.887966  lr: 1.00e-03\n",
            "Epoch 027/60 - train: 0.884934  val: 0.887930  lr: 1.00e-03\n",
            "Epoch 028/60 - train: 0.885167  val: 0.887875  lr: 1.00e-03\n",
            "Epoch 029/60 - train: 0.885412  val: 0.887856  lr: 1.00e-03\n",
            "Epoch 030/60 - train: 0.884974  val: 0.887825  lr: 1.00e-03\n",
            "Epoch 031/60 - train: 0.884385  val: 0.887809  lr: 1.00e-03\n",
            "Epoch 032/60 - train: 0.885352  val: 0.887772  lr: 1.00e-03\n",
            "Epoch 033/60 - train: 0.884831  val: 0.887752  lr: 1.00e-03\n",
            "Epoch 034/60 - train: 0.884199  val: 0.887750  lr: 1.00e-03\n",
            "Epoch 035/60 - train: 0.885066  val: 0.887718  lr: 1.00e-03\n",
            "Epoch 036/60 - train: 0.884415  val: 0.887695  lr: 1.00e-03\n",
            "Epoch 037/60 - train: 0.884514  val: 0.887683  lr: 1.00e-03\n",
            "Epoch 038/60 - train: 0.884533  val: 0.887657  lr: 1.00e-03\n",
            "Epoch 039/60 - train: 0.884625  val: 0.887665  lr: 1.00e-03\n",
            "Epoch 040/60 - train: 0.884444  val: 0.887640  lr: 1.00e-03\n",
            "Epoch 041/60 - train: 0.884513  val: 0.887620  lr: 1.00e-03\n",
            "Epoch 042/60 - train: 0.884531  val: 0.887612  lr: 1.00e-03\n",
            "Epoch 043/60 - train: 0.884420  val: 0.887598  lr: 1.00e-03\n",
            "Epoch 044/60 - train: 0.884766  val: 0.887578  lr: 1.00e-03\n",
            "Epoch 045/60 - train: 0.884060  val: 0.887562  lr: 1.00e-03\n",
            "Epoch 046/60 - train: 0.884574  val: 0.887552  lr: 1.00e-03\n",
            "Epoch 047/60 - train: 0.884847  val: 0.887538  lr: 5.00e-04\n",
            "Epoch 048/60 - train: 0.883774  val: 0.887534  lr: 5.00e-04\n",
            "Epoch 049/60 - train: 0.884522  val: 0.887528  lr: 5.00e-04\n",
            "Epoch 050/60 - train: 0.884126  val: 0.887525  lr: 5.00e-04\n",
            "Epoch 051/60 - train: 0.884433  val: 0.887521  lr: 5.00e-04\n",
            "Epoch 052/60 - train: 0.884662  val: 0.887513  lr: 5.00e-04\n",
            "Epoch 053/60 - train: 0.884610  val: 0.887501  lr: 5.00e-04\n",
            "Epoch 054/60 - train: 0.884734  val: 0.887505  lr: 5.00e-04\n",
            "Epoch 055/60 - train: 0.884068  val: 0.887498  lr: 2.50e-04\n",
            "Epoch 056/60 - train: 0.884028  val: 0.887493  lr: 2.50e-04\n",
            "Epoch 057/60 - train: 0.884253  val: 0.887490  lr: 2.50e-04\n",
            "Epoch 058/60 - train: 0.884640  val: 0.887491  lr: 2.50e-04\n",
            "Epoch 059/60 - train: 0.884153  val: 0.887487  lr: 2.50e-04\n",
            "Epoch 060/60 - train: 0.884810  val: 0.887483  lr: 2.50e-04\n"
          ]
        }
      ],
      "source": [
        "# Train\n",
        "num_epochs = 60\n",
        "train_losses, val_losses = [], []\n",
        "best_val = float('inf')\n",
        "bad = 0\n",
        "patience = 12\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    tr = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    va = validate_epoch(model, val_loader, criterion, device)\n",
        "    scheduler.step(va)\n",
        "    train_losses.append(tr)\n",
        "    val_losses.append(va)\n",
        "\n",
        "    if va < best_val - 1e-6:\n",
        "        best_val = va\n",
        "        bad = 0\n",
        "        torch.save(model.state_dict(), 'best_baseline_simple.pth')\n",
        "    else:\n",
        "        bad += 1\n",
        "        if bad >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch}\")\n",
        "            break\n",
        "\n",
        "    lr = optimizer.param_groups[0]['lr']\n",
        "    print(f\"Epoch {epoch:03d}/{num_epochs} - train: {tr:.6f}  val: {va:.6f}  lr: {lr:.2e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Component_1: MSE=0.158437, R2=0.0001\n",
            "Component_2: MSE=0.068778, R2=0.0012\n",
            "Component_3: MSE=0.095149, R2=0.0011\n",
            "Component_4: MSE=0.155462, R2=0.0003\n",
            "Overall: MSE=0.119457, R2=0.1138\n"
          ]
        }
      ],
      "source": [
        "# Evaluate\n",
        "model.load_state_dict(torch.load('best_baseline_simple.pth', map_location=device))\n",
        "preds, tgts = evaluate_model(model, test_loader, device)\n",
        "\n",
        "comp_names = ['Component_1', 'Component_2', 'Component_3', 'Component_4']\n",
        "metrics = {}\n",
        "for i, name in enumerate(comp_names):\n",
        "    mse = mean_squared_error(tgts[:, i], preds[:, i])\n",
        "    r2 = r2_score(tgts[:, i], preds[:, i])\n",
        "    metrics[name] = {'MSE': float(mse), 'R2': float(r2)}\n",
        "    print(f\"{name}: MSE={mse:.6f}, R2={r2:.4f}\")\n",
        "\n",
        "overall_mse = mean_squared_error(tgts, preds)\n",
        "overall_r2 = r2_score(tgts.flatten(), preds.flatten())\n",
        "print(f\"Overall: MSE={overall_mse:.6f}, R2={overall_r2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
