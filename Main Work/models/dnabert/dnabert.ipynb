{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DNABERT-like Promoter Classifier (PyTorch)\n",
        "\n",
        "A minimal, self-contained notebook that trains a DNABERT-style classifier on promoter sequences:\n",
        "- Tokenises DNA into overlapping k-mers (k=6 by default)\n",
        "- [CLS] + tokens + [SEP]\n",
        "- Token + positional embeddings\n",
        "- Transformer encoder stack\n",
        "- Classification head on [CLS] to predict 4-component probabilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch: 2.5.1\n",
            "CUDA: False\n",
            "MPS: True\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import math\n",
        "from itertools import product\n",
        "from typing import List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "print(f\"MPS: {getattr(torch.backends, 'mps', None) is not None and torch.backends.mps.is_available()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data utilities (inline to keep notebook self-contained)\n",
        "class PromoterDataset(Dataset):\n",
        "    def __init__(self, sequences: list, targets: np.ndarray, max_length: int = 600):\n",
        "        self.sequences = sequences\n",
        "        self.targets = targets\n",
        "        self.max_length = max_length\n",
        "        self.dna_dict = {\"A\": 0, \"T\": 1, \"G\": 2, \"C\": 3, \"N\": 4}\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "    def encode_sequence(self, sequence: str) -> np.ndarray:\n",
        "        seq = sequence\n",
        "        if len(seq) > self.max_length:\n",
        "            seq = seq[: self.max_length]\n",
        "        else:\n",
        "            seq = seq + \"N\" * (self.max_length - len(seq))\n",
        "        encoded = np.array([self.dna_dict.get(base.upper(), 4) for base in seq])\n",
        "        one_hot = np.zeros((self.max_length, 5), dtype=np.float32)\n",
        "        one_hot[np.arange(self.max_length), encoded] = 1.0\n",
        "        return one_hot.T\n",
        "    def __getitem__(self, idx: int):\n",
        "        sequence = self.encode_sequence(self.sequences[idx])\n",
        "        target = self.targets[idx].astype(np.float32)\n",
        "        total = float(np.sum(target))\n",
        "        if total <= 0:\n",
        "            target = np.ones_like(target, dtype=np.float32) / target.shape[0]\n",
        "        else:\n",
        "            target = target / total\n",
        "        return {\"sequence\": torch.FloatTensor(sequence), \"target\": torch.FloatTensor(target)}\n",
        "\n",
        "def load_and_prepare_data(file_path: str):\n",
        "    df = pd.read_csv(file_path)\n",
        "    prob_cols = [\"Component_1_Probability\", \"Component_2_Probability\", \"Component_3_Probability\", \"Component_4_Probability\"]\n",
        "    df = df.dropna(subset=[\"ProSeq\"]).dropna(subset=prob_cols)\n",
        "    sequences = df[\"ProSeq\"].tolist()\n",
        "    targets = df[prob_cols].values\n",
        "    valid_sequences = []\n",
        "    valid_targets = []\n",
        "    for i, seq in enumerate(sequences):\n",
        "        if isinstance(seq, str) and len(seq) > 0:\n",
        "            valid_sequences.append(seq)\n",
        "            valid_targets.append(targets[i])\n",
        "    return valid_sequences, np.array(valid_targets, dtype=np.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DNABERT-like model and tokenisation\n",
        "SPECIALS = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\"]\n",
        "\n",
        "def build_kmer_vocab(k: int):\n",
        "    bases = [\"A\", \"C\", \"G\", \"T\"]\n",
        "    kmers = [\"\".join(p) for p in product(bases, repeat=k)]\n",
        "    vocab = SPECIALS + kmers\n",
        "    stoi = {t: i for i, t in enumerate(vocab)}\n",
        "    itos = {i: t for t, i in stoi.items()}\n",
        "    return vocab, stoi, itos\n",
        "\n",
        "def seq_to_kmers(seq: str, k: int) -> List[str]:\n",
        "    seq = seq.upper()\n",
        "    toks: List[str] = []\n",
        "    for i in range(len(seq) - k + 1):\n",
        "        kmer = seq[i:i+k]\n",
        "        if any(c not in \"ACGT\" for c in kmer):\n",
        "            toks.append(\"[UNK]\")\n",
        "        else:\n",
        "            toks.append(kmer)\n",
        "    return [\"[CLS]\"] + toks + [\"[SEP]\"]\n",
        "\n",
        "def encode_batch(seqs: List[str], k: int, stoi: dict, max_len: Optional[int] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    tokenised = [seq_to_kmers(s, k) for s in seqs]\n",
        "    if max_len is None:\n",
        "        max_len = max(len(t) for t in tokenised)\n",
        "    pad_id = stoi[\"[PAD]\"]\n",
        "    unk_id = stoi[\"[UNK]\"]\n",
        "    input_ids = []\n",
        "    attn = []\n",
        "    for toks in tokenised:\n",
        "        ids = [stoi.get(t, unk_id) for t in toks[:max_len]]\n",
        "        mask = [1] * len(ids)\n",
        "        if len(ids) < max_len:\n",
        "            pad_n = max_len - len(ids)\n",
        "            ids += [pad_id] * pad_n\n",
        "            mask += [0] * pad_n\n",
        "        input_ids.append(ids)\n",
        "        attn.append(mask)\n",
        "    return torch.tensor(input_ids, dtype=torch.long), torch.tensor(attn, dtype=torch.bool)\n",
        "\n",
        "def onehot5_to_strings(x: torch.Tensor) -> List[str]:\n",
        "    assert x.ndim == 3 and x.size(1) == 5, \"expected (B,5,L)\"\n",
        "    idx = x.argmax(dim=1)\n",
        "    lut = {0: \"A\", 1: \"T\", 2: \"G\", 3: \"C\", 4: \"N\"}\n",
        "    return [\"\".join(lut[int(i)] for i in row) for row in idx]\n",
        "\n",
        "class DNABertClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        k: int = 6,\n",
        "        num_labels: int = 4,\n",
        "        hidden_size: int = 256,\n",
        "        num_layers: int = 6,\n",
        "        num_heads: int = 8,\n",
        "        ffn_size: Optional[int] = None,\n",
        "        dropout: float = 0.1,\n",
        "        max_position_embeddings: int = 1024,\n",
        "        vocab: Optional[List[str]] = None,\n",
        "        stoi: Optional[dict] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.k = k\n",
        "        self.num_labels = num_labels\n",
        "        if vocab is None or stoi is None:\n",
        "            vocab, stoi, _ = build_kmer_vocab(k)\n",
        "        self.stoi = stoi\n",
        "        self.vocab_size = len(vocab)\n",
        "        if ffn_size is None:\n",
        "            ffn_size = 4 * hidden_size\n",
        "        self.token_embeddings = nn.Embedding(self.vocab_size, hidden_size)\n",
        "        self.pos_embeddings = nn.Embedding(max_position_embeddings, hidden_size)\n",
        "        self.emb_layer_norm = nn.LayerNorm(hidden_size)\n",
        "        self.emb_dropout = nn.Dropout(dropout)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_size,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=ffn_size,\n",
        "            dropout=dropout,\n",
        "            activation=\"gelu\",\n",
        "            batch_first=True,\n",
        "            norm_first=False,\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size, num_labels),\n",
        "        )\n",
        "        self.apply(self._init_weights)\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, (nn.Linear, nn.Embedding)):\n",
        "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.ones_(m.weight)\n",
        "            nn.init.zeros_(m.bias)\n",
        "    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        bsz, seqlen = input_ids.shape\n",
        "        device = input_ids.device\n",
        "        if attention_mask is None:\n",
        "            attention_mask = input_ids.ne(self.stoi[\"[PAD]\"])\n",
        "        pos_ids = torch.arange(seqlen, device=device).unsqueeze(0).expand(bsz, seqlen)\n",
        "        x = self.token_embeddings(input_ids) + self.pos_embeddings(pos_ids)\n",
        "        x = self.emb_layer_norm(self.emb_dropout(x))\n",
        "        x = self.encoder(x, src_key_padding_mask=~attention_mask.bool())\n",
        "        cls = x[:, 0]\n",
        "        logits = self.classifier(cls)\n",
        "        return logits\n",
        "\n",
        "def prepare_inputs_from_onehot(onehot_batch: torch.Tensor, k: int, stoi: dict, max_len: Optional[int] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    seqs = onehot5_to_strings(onehot_batch)\n",
        "    input_ids, attn = encode_batch(seqs, k, stoi, max_len=max_len)\n",
        "    return input_ids.to(onehot_batch.device), attn.to(onehot_batch.device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training helpers\n",
        "\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total = 0.0\n",
        "    for batch in train_loader:\n",
        "        onehot = batch['sequence'].to(device)\n",
        "        targets = batch['target'].to(device)\n",
        "        input_ids, attn = prepare_inputs_from_onehot(onehot, k=model.k, stoi=model.stoi, max_len=(onehot.shape[-1]-model.k+1)+2)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_ids, attn)\n",
        "        log_probs = F.log_softmax(logits, dim=1)\n",
        "        loss = criterion(log_probs, targets)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        total += loss.item()\n",
        "    return total / max(1, len(train_loader))\n",
        "\n",
        "def validate_epoch(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            onehot = batch['sequence'].to(device)\n",
        "            targets = batch['target'].to(device)\n",
        "            input_ids, attn = prepare_inputs_from_onehot(onehot, k=model.k, stoi=model.stoi, max_len=(onehot.shape[-1]-model.k+1)+2)\n",
        "            logits = model(input_ids, attn)\n",
        "            log_probs = F.log_softmax(logits, dim=1)\n",
        "            loss = criterion(log_probs, targets)\n",
        "            total += loss.item()\n",
        "    return total / max(1, len(val_loader))\n",
        "\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    preds, targs = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            onehot = batch['sequence'].to(device)\n",
        "            targets = batch['target'].to(device)\n",
        "            input_ids, attn = prepare_inputs_from_onehot(onehot, k=model.k, stoi=model.stoi, max_len=(onehot.shape[-1]-model.k+1)+2)\n",
        "            logits = model(input_ids, attn)\n",
        "            probs = torch.softmax(logits, dim=1)\n",
        "            preds.append(probs.cpu().numpy())\n",
        "            targs.append(targets.cpu().numpy())\n",
        "    return np.vstack(preds), np.vstack(targs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5590 1398 1747\n"
          ]
        }
      ],
      "source": [
        "# Load data and build loaders\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "csv_path = \"../../Processed-Data/ProSeq_with_4component_analysis.csv\"\n",
        "sequences, targets = load_and_prepare_data(csv_path)\n",
        "\n",
        "labels = np.argmax(targets, axis=1)\n",
        "train_seq, test_seq, train_targets, test_targets = train_test_split(\n",
        "    sequences, targets, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "train_labels = np.argmax(train_targets, axis=1)\n",
        "train_seq, val_seq, train_targets, val_targets = train_test_split(\n",
        "    train_seq, train_targets, test_size=0.2, random_state=42, stratify=train_labels\n",
        ")\n",
        "\n",
        "train_ds = PromoterDataset(train_seq, train_targets)\n",
        "val_ds = PromoterDataset(val_seq, val_targets)\n",
        "test_ds = PromoterDataset(test_seq, test_targets)\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "print(len(train_ds), len(val_ds), len(test_ds))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/jaydenthai/Dev/University Work/2025sem2/DS-Research-Project-Tumor-Expression/.conda/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on mps...\n"
          ]
        },
        {
          "ename": "NotImplementedError",
          "evalue": "The operator 'aten::_nested_tensor_from_mask_left_aligned' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m     21\u001b[39m     tr = train_epoch(model, train_loader, criterion, optimizer, device)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     va = \u001b[43mvalidate_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     scheduler.step(va)\n\u001b[32m     24\u001b[39m     train_losses.append(tr)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mvalidate_epoch\u001b[39m\u001b[34m(model, val_loader, criterion, device)\u001b[39m\n\u001b[32m     26\u001b[39m targets = batch[\u001b[33m'\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m'\u001b[39m].to(device)\n\u001b[32m     27\u001b[39m input_ids, attn = prepare_inputs_from_onehot(onehot, k=model.k, stoi=model.stoi, max_len=(onehot.shape[-\u001b[32m1\u001b[39m]-model.k+\u001b[32m1\u001b[39m)+\u001b[32m2\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m log_probs = F.log_softmax(logits, dim=\u001b[32m1\u001b[39m)\n\u001b[32m     30\u001b[39m loss = criterion(log_probs, targets)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/University Work/2025sem2/DS-Research-Project-Tumor-Expression/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/University Work/2025sem2/DS-Research-Project-Tumor-Expression/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 108\u001b[39m, in \u001b[36mDNABertClassifier.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask)\u001b[39m\n\u001b[32m    106\u001b[39m x = \u001b[38;5;28mself\u001b[39m.token_embeddings(input_ids) + \u001b[38;5;28mself\u001b[39m.pos_embeddings(pos_ids)\n\u001b[32m    107\u001b[39m x = \u001b[38;5;28mself\u001b[39m.emb_layer_norm(\u001b[38;5;28mself\u001b[39m.emb_dropout(x))\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m~\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbool\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[38;5;28mcls\u001b[39m = x[:, \u001b[32m0\u001b[39m]\n\u001b[32m    110\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.classifier(\u001b[38;5;28mcls\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/University Work/2025sem2/DS-Research-Project-Tumor-Expression/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/University Work/2025sem2/DS-Research-Project-Tumor-Expression/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/University Work/2025sem2/DS-Research-Project-Tumor-Expression/.conda/lib/python3.11/site-packages/torch/nn/modules/transformer.py:454\u001b[39m, in \u001b[36mTransformerEncoder.forward\u001b[39m\u001b[34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[39m\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m src_key_padding_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    451\u001b[39m     why_not_sparsity_fast_path = \u001b[33m\"\u001b[39m\u001b[33msrc_key_padding_mask was None\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    452\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m    453\u001b[39m     (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmask_check\u001b[39m\u001b[33m\"\u001b[39m)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mask_check\n\u001b[32m--> \u001b[39m\u001b[32m454\u001b[39m ) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nested_tensor_from_mask_left_aligned\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m    \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogical_not\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    457\u001b[39m     why_not_sparsity_fast_path = \u001b[33m\"\u001b[39m\u001b[33mmask_check enabled, and src and src_key_padding_mask was not left aligned\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m output.is_nested:\n",
            "\u001b[31mNotImplementedError\u001b[39m: The operator 'aten::_nested_tensor_from_mask_left_aligned' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
          ]
        }
      ],
      "source": [
        "# Train DNABERT-like model\n",
        "if getattr(torch.backends, 'mps', None) is not None and torch.backends.mps.is_available():\n",
        "    device = torch.device('mps')\n",
        "else:\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = DNABertClassifier(k=6, num_labels=4, hidden_size=256, num_layers=6, num_heads=8, max_position_embeddings=1024)\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.KLDivLoss(reduction='batchmean')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=8, factor=0.5)\n",
        "\n",
        "num_epochs = 30\n",
        "train_losses, val_losses = [], []\n",
        "best_val = float('inf')\n",
        "bad_epochs = 0\n",
        "max_bad_epochs = 10\n",
        "print(f\"Training on {device}...\")\n",
        "for epoch in range(num_epochs):\n",
        "    tr = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    va = validate_epoch(model, val_loader, criterion, device)\n",
        "    scheduler.step(va)\n",
        "    train_losses.append(tr)\n",
        "    val_losses.append(va)\n",
        "    if va < best_val - 1e-6:\n",
        "        best_val = va\n",
        "        bad_epochs = 0\n",
        "        torch.save(model.state_dict(), 'best_dnabert_like.pth')\n",
        "    else:\n",
        "        bad_epochs += 1\n",
        "        if bad_epochs >= max_bad_epochs:\n",
        "            print(f\"Early stop at epoch {epoch+1}\")\n",
        "            break\n",
        "    print(f\"Epoch {epoch+1:03d}/{num_epochs} - train {tr:.6f} - val {va:.6f} - lr {optimizer.param_groups[0]['lr']:.2e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Evaluation and quick plots\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# reload best\n",
        "model.load_state_dict(torch.load('best_dnabert_like.pth', map_location=device))\n",
        "\n",
        "predictions, true_targets = evaluate_model(model, test_loader, device)\n",
        "\n",
        "component_names = ['Component_1', 'Component_2', 'Component_3', 'Component_4']\n",
        "metrics = {}\n",
        "for i, name in enumerate(component_names):\n",
        "    mse = mean_squared_error(true_targets[:, i], predictions[:, i])\n",
        "    r2 = r2_score(true_targets[:, i], predictions[:, i])\n",
        "    metrics[name] = {'MSE': float(mse), 'R2': float(r2)}\n",
        "\n",
        "overall_mse = mean_squared_error(true_targets, predictions)\n",
        "overall_r2 = r2_score(true_targets.flatten(), predictions.flatten())\n",
        "metrics['Overall'] = {'MSE': float(overall_mse), 'R2': float(overall_r2)}\n",
        "\n",
        "print(metrics)\n",
        "\n",
        "# basic training curve\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(train_losses, label='train')\n",
        "plt.plot(val_losses, label='val')\n",
        "plt.legend(); plt.xlabel('epoch'); plt.ylabel('loss'); plt.title('DNABERT-like training'); plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Single-sequence prediction helper\n",
        "def predict_component_probabilities_dnabert(model: DNABertClassifier, sequence: str, device):\n",
        "    model.eval()\n",
        "    seqs = [sequence]\n",
        "    max_len = (len(sequence)-model.k+1)+2 if len(sequence) >= model.k else model.k+2\n",
        "    input_ids, attn = encode_batch(seqs, model.k, model.stoi, max_len=max_len)\n",
        "    input_ids = input_ids.to(device)\n",
        "    attn = attn.to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids, attn)\n",
        "        probs = torch.softmax(logits, dim=1)[0].cpu().numpy()\n",
        "    pred_comp = int(np.argmax(probs) + 1)\n",
        "    conf = float(np.max(probs))\n",
        "    return {\n",
        "        'component_1_prob': float(probs[0]),\n",
        "        'component_2_prob': float(probs[1]),\n",
        "        'component_3_prob': float(probs[2]),\n",
        "        'component_4_prob': float(probs[3]),\n",
        "        'predicted_component': pred_comp,\n",
        "        'confidence': conf,\n",
        "    }\n",
        "\n",
        "# Example\n",
        "sample = sequences[0]\n",
        "print(predict_component_probabilities_dnabert(model, sample, device))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
