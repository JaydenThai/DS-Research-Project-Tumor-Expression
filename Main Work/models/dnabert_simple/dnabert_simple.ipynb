{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simple DNABERT-like Promoter Classifier (PyTorch)\n",
        "\n",
        "A lightweight DNABERT-style classifier optimized for faster training and lower memory usage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch: 2.5.1\n",
            "CUDA: False\n",
            "MPS: True\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "from itertools import product\n",
        "from typing import List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "print(f\"MPS: {getattr(torch.backends, 'mps', None) is not None and torch.backends.mps.is_available()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data utilities\n",
        "class PromoterDataset(Dataset):\n",
        "    def __init__(self, sequences: list, targets: np.ndarray, max_length: int = 600):\n",
        "        self.sequences = sequences\n",
        "        self.targets = targets\n",
        "        self.max_length = max_length\n",
        "        self.dna_dict = {\"A\": 0, \"T\": 1, \"G\": 2, \"C\": 3, \"N\": 4}\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "    def encode_sequence(self, sequence: str) -> np.ndarray:\n",
        "        seq = sequence\n",
        "        if len(seq) > self.max_length:\n",
        "            seq = seq[: self.max_length]\n",
        "        else:\n",
        "            seq = seq + \"N\" * (self.max_length - len(seq))\n",
        "        encoded = np.array([self.dna_dict.get(base.upper(), 4) for base in seq])\n",
        "        one_hot = np.zeros((self.max_length, 5), dtype=np.float32)\n",
        "        one_hot[np.arange(self.max_length), encoded] = 1.0\n",
        "        return one_hot.T\n",
        "    def __getitem__(self, idx: int):\n",
        "        sequence = self.encode_sequence(self.sequences[idx])\n",
        "        target = self.targets[idx].astype(np.float32)\n",
        "        total = float(np.sum(target))\n",
        "        if total <= 0:\n",
        "            target = np.ones_like(target, dtype=np.float32) / target.shape[0]\n",
        "        else:\n",
        "            target = target / total\n",
        "        return {\"sequence\": torch.FloatTensor(sequence), \"target\": torch.FloatTensor(target)}\n",
        "\n",
        "def load_and_prepare_data(file_path: str):\n",
        "    df = pd.read_csv(file_path)\n",
        "    prob_cols = [\"Component_1_Probability\", \"Component_2_Probability\", \"Component_3_Probability\", \"Component_4_Probability\"]\n",
        "    df = df.dropna(subset=[\"ProSeq\"]).dropna(subset=prob_cols)\n",
        "    sequences = df[\"ProSeq\"].tolist()\n",
        "    targets = df[prob_cols].values\n",
        "    valid_sequences = []\n",
        "    valid_targets = []\n",
        "    for i, seq in enumerate(sequences):\n",
        "        if isinstance(seq, str) and len(seq) > 0:\n",
        "            valid_sequences.append(seq)\n",
        "            valid_targets.append(targets[i])\n",
        "    return valid_sequences, np.array(valid_targets, dtype=np.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenisation helpers\n",
        "SPECIALS = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\"]\n",
        "\n",
        "def build_kmer_vocab(k: int):\n",
        "    bases = [\"A\", \"C\", \"G\", \"T\"]\n",
        "    from itertools import product as _product\n",
        "    kmers = [\"\".join(p) for p in _product(bases, repeat=k)]\n",
        "    vocab = SPECIALS + kmers\n",
        "    stoi = {t: i for i, t in enumerate(vocab)}\n",
        "    itos = {i: t for t, i in stoi.items()}\n",
        "    return vocab, stoi, itos\n",
        "\n",
        "def seq_to_kmers(seq: str, k: int) -> List[str]:\n",
        "    seq = seq.upper()\n",
        "    toks: List[str] = []\n",
        "    for i in range(len(seq) - k + 1):\n",
        "        kmer = seq[i:i+k]\n",
        "        if any(c not in \"ACGT\" for c in kmer):\n",
        "            toks.append(\"[UNK]\")\n",
        "        else:\n",
        "            toks.append(kmer)\n",
        "    return [\"[CLS]\"] + toks + [\"[SEP]\"]\n",
        "\n",
        "def encode_batch(seqs: List[str], k: int, stoi: dict, max_len: Optional[int] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    tokenised = [seq_to_kmers(s, k) for s in seqs]\n",
        "    if max_len is None:\n",
        "        max_len = max(len(t) for t in tokenised)\n",
        "    pad_id = stoi[\"[PAD]\"]\n",
        "    unk_id = stoi[\"[UNK]\"]\n",
        "    input_ids = []\n",
        "    attn = []\n",
        "    for toks in tokenised:\n",
        "        ids = [stoi.get(t, unk_id) for t in toks[:max_len]]\n",
        "        mask = [1] * len(ids)\n",
        "        if len(ids) < max_len:\n",
        "            pad_n = max_len - len(ids)\n",
        "            ids += [pad_id] * pad_n\n",
        "            mask += [0] * pad_n\n",
        "        input_ids.append(ids)\n",
        "        attn.append(mask)\n",
        "    return torch.tensor(input_ids, dtype=torch.long), torch.tensor(attn, dtype=torch.bool)\n",
        "\n",
        "def onehot5_to_strings(x: torch.Tensor) -> List[str]:\n",
        "    assert x.ndim == 3 and x.size(1) == 5, \"expected (B,5,L)\"\n",
        "    idx = x.argmax(dim=1)\n",
        "    lut = {0: \"A\", 1: \"T\", 2: \"G\", 3: \"C\", 4: \"N\"}\n",
        "    return [\"\".join(lut[int(i)] for i in row) for row in idx]\n",
        "\n",
        "def prepare_inputs_from_onehot(onehot_batch: torch.Tensor, k: int, stoi: dict, max_len: Optional[int] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    seqs = onehot5_to_strings(onehot_batch)\n",
        "    input_ids, attn = encode_batch(seqs, k, stoi, max_len=max_len)\n",
        "    return input_ids.to(onehot_batch.device), attn.to(onehot_batch.device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple model\n",
        "class SimpleDNABert(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        k: int = 6,\n",
        "        num_labels: int = 4,\n",
        "        hidden_size: int = 128,\n",
        "        num_layers: int = 2,\n",
        "        num_heads: int = 4,\n",
        "        dropout: float = 0.1,\n",
        "        max_position_embeddings: int = 1024,\n",
        "        vocab: Optional[List[str]] = None,\n",
        "        stoi: Optional[dict] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.k = k\n",
        "        self.num_labels = num_labels\n",
        "        if vocab is None or stoi is None:\n",
        "            vocab, stoi, _ = build_kmer_vocab(k)\n",
        "        self.stoi = stoi\n",
        "        self.vocab_size = len(vocab)\n",
        "\n",
        "        self.token_embeddings = nn.Embedding(self.vocab_size, hidden_size)\n",
        "        self.pos_embeddings = nn.Embedding(max_position_embeddings, hidden_size)\n",
        "        self.emb_layer_norm = nn.LayerNorm(hidden_size)\n",
        "        self.emb_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_size,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=hidden_size * 2,\n",
        "            dropout=dropout,\n",
        "            activation=\"gelu\",\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size, num_labels),\n",
        "        )\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, (nn.Linear, nn.Embedding)):\n",
        "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.ones_(m.weight)\n",
        "            nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        bsz, seqlen = input_ids.shape\n",
        "        if attention_mask is None:\n",
        "            attention_mask = input_ids.ne(self.stoi[\"[PAD]\"])\n",
        "        pos_ids = torch.arange(seqlen, device=input_ids.device).unsqueeze(0).expand(bsz, seqlen)\n",
        "        x = self.token_embeddings(input_ids) + self.pos_embeddings(pos_ids)\n",
        "        x = self.emb_layer_norm(self.emb_dropout(x))\n",
        "        x = self.encoder(x, src_key_padding_mask=~attention_mask.bool())\n",
        "        cls = x[:, 0]\n",
        "        logits = self.classifier(cls)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training helpers (reuse DNABERT pipeline with onehot->kmer conversion)\n",
        "\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total = 0.0\n",
        "    for batch in train_loader:\n",
        "        onehot = batch['sequence'].to(device)\n",
        "        targets = batch['target'].to(device)\n",
        "        input_ids, attn = prepare_inputs_from_onehot(onehot, k=model.k, stoi=model.stoi, max_len=(onehot.shape[-1]-model.k+1)+2)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_ids, attn)\n",
        "        log_probs = F.log_softmax(logits, dim=1)\n",
        "        loss = criterion(log_probs, targets)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        total += loss.item()\n",
        "    return total / max(1, len(train_loader))\n",
        "\n",
        "\n",
        "def validate_epoch(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            onehot = batch['sequence'].to(device)\n",
        "            targets = batch['target'].to(device)\n",
        "            input_ids, attn = prepare_inputs_from_onehot(onehot, k=model.k, stoi=model.stoi, max_len=(onehot.shape[-1]-model.k+1)+2)\n",
        "            logits = model(input_ids, attn)\n",
        "            log_probs = F.log_softmax(logits, dim=1)\n",
        "            loss = criterion(log_probs, targets)\n",
        "            total += loss.item()\n",
        "    return total / max(1, len(val_loader))\n",
        "\n",
        "\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    preds, targs = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            onehot = batch['sequence'].to(device)\n",
        "            targets = batch['target'].to(device)\n",
        "            input_ids, attn = prepare_inputs_from_onehot(onehot, k=model.k, stoi=model.stoi, max_len=(onehot.shape[-1]-model.k+1)+2)\n",
        "            logits = model(input_ids, attn)\n",
        "            probs = torch.softmax(logits, dim=1)\n",
        "            preds.append(probs.cpu().numpy())\n",
        "            targs.append(targets.cpu().numpy())\n",
        "    return np.vstack(preds), np.vstack(targs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5590 1398 1747\n"
          ]
        }
      ],
      "source": [
        "# Load data and build loaders\n",
        "csv_path = \"../../Processed-Data/ProSeq_with_4component_analysis.csv\"\n",
        "sequences, targets = load_and_prepare_data(csv_path)\n",
        "\n",
        "labels = np.argmax(targets, axis=1)\n",
        "train_seq, test_seq, train_targets, test_targets = train_test_split(\n",
        "    sequences, targets, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "train_labels = np.argmax(train_targets, axis=1)\n",
        "train_seq, val_seq, train_targets, val_targets = train_test_split(\n",
        "    train_seq, train_targets, test_size=0.2, random_state=42, stratify=train_labels\n",
        ")\n",
        "\n",
        "train_ds = PromoterDataset(train_seq, train_targets)\n",
        "val_ds = PromoterDataset(val_seq, val_targets)\n",
        "test_ds = PromoterDataset(test_seq, test_targets)\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "print(len(train_ds), len(val_ds), len(test_ds))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on mps...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     tr = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m     va = validate_epoch(model, val_loader, criterion, device)\n\u001b[32m     23\u001b[39m     scheduler.step(va)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, train_loader, criterion, optimizer, device)\u001b[39m\n\u001b[32m      7\u001b[39m onehot = batch[\u001b[33m'\u001b[39m\u001b[33msequence\u001b[39m\u001b[33m'\u001b[39m].to(device)\n\u001b[32m      8\u001b[39m targets = batch[\u001b[33m'\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m'\u001b[39m].to(device)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m input_ids, attn = \u001b[43mprepare_inputs_from_onehot\u001b[49m\u001b[43m(\u001b[49m\u001b[43monehot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstoi\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstoi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43monehot\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m-\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mk\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m optimizer.zero_grad()\n\u001b[32m     11\u001b[39m logits = model(input_ids, attn)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mprepare_inputs_from_onehot\u001b[39m\u001b[34m(onehot_batch, k, stoi, max_len)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprepare_inputs_from_onehot\u001b[39m(onehot_batch: torch.Tensor, k: \u001b[38;5;28mint\u001b[39m, stoi: \u001b[38;5;28mdict\u001b[39m, max_len: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m) -> Tuple[torch.Tensor, torch.Tensor]:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     seqs = \u001b[43monehot5_to_strings\u001b[49m\u001b[43m(\u001b[49m\u001b[43monehot_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     input_ids, attn = encode_batch(seqs, k, stoi, max_len=max_len)\n\u001b[32m     52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m input_ids.to(onehot_batch.device), attn.to(onehot_batch.device)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36monehot5_to_strings\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     45\u001b[39m idx = x.argmax(dim=\u001b[32m1\u001b[39m)\n\u001b[32m     46\u001b[39m lut = {\u001b[32m0\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mA\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mT\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m2\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mG\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m3\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mC\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m4\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mN\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlut\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     45\u001b[39m idx = x.argmax(dim=\u001b[32m1\u001b[39m)\n\u001b[32m     46\u001b[39m lut = {\u001b[32m0\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mA\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mT\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m2\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mG\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m3\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mC\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m4\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mN\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(lut[\u001b[38;5;28mint\u001b[39m(i)] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m row) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m idx]\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     45\u001b[39m idx = x.argmax(dim=\u001b[32m1\u001b[39m)\n\u001b[32m     46\u001b[39m lut = {\u001b[32m0\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mA\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mT\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m2\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mG\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m3\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mC\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m4\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mN\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(lut[\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m row) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m idx]\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Train SimpleDNABert\n",
        "if getattr(torch.backends, 'mps', None) is not None and torch.backends.mps.is_available():\n",
        "    device = torch.device('mps')\n",
        "else:\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = SimpleDNABert(k=6, num_labels=4, hidden_size=128, num_layers=2, num_heads=4, max_position_embeddings=1024)\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.KLDivLoss(reduction='batchmean')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=6, factor=0.5)\n",
        "\n",
        "num_epochs = 15\n",
        "train_losses, val_losses = [], []\n",
        "best_val = float('inf')\n",
        "bad_epochs = 0\n",
        "max_bad_epochs = 6\n",
        "print(f\"Training on {device}...\")\n",
        "for epoch in range(num_epochs):\n",
        "    tr = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    va = validate_epoch(model, val_loader, criterion, device)\n",
        "    scheduler.step(va)\n",
        "    train_losses.append(tr)\n",
        "    val_losses.append(va)\n",
        "    if va < best_val - 1e-6:\n",
        "        best_val = va\n",
        "        bad_epochs = 0\n",
        "        torch.save(model.state_dict(), 'best_simple_dnabert.pth')\n",
        "    else:\n",
        "        bad_epochs += 1\n",
        "        if bad_epochs >= max_bad_epochs:\n",
        "            print(f\"Early stop at epoch {epoch+1}\")\n",
        "            break\n",
        "    print(f\"Epoch {epoch+1:03d}/{num_epochs} - train {tr:.6f} - val {va:.6f} - lr {optimizer.param_groups[0]['lr']:.2e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate and quick plot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model.load_state_dict(torch.load('best_simple_dnabert.pth', map_location=device))\n",
        "\n",
        "predictions, true_targets = evaluate_model(model, test_loader, device)\n",
        "\n",
        "component_names = ['Component_1', 'Component_2', 'Component_3', 'Component_4']\n",
        "metrics = {}\n",
        "for i, name in enumerate(component_names):\n",
        "    mse = mean_squared_error(true_targets[:, i], predictions[:, i])\n",
        "    r2 = r2_score(true_targets[:, i], predictions[:, i])\n",
        "    metrics[name] = {'MSE': float(mse), 'R2': float(r2)}\n",
        "\n",
        "overall_mse = mean_squared_error(true_targets, predictions)\n",
        "overall_r2 = r2_score(true_targets.flatten(), predictions.flatten())\n",
        "metrics['Overall'] = {'MSE': float(overall_mse), 'R2': float(overall_r2)}\n",
        "\n",
        "print(metrics)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(train_losses, label='train')\n",
        "plt.plot(val_losses, label='val')\n",
        "plt.legend(); plt.xlabel('epoch'); plt.ylabel('loss'); plt.title('Simple DNABERT training'); plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
