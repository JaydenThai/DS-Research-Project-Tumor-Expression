{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c3f1cbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "94b2d798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root to Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from src.utils.data import load_and_prepare_data_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "35b5c64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8304 sequences with 4 unique classes\n",
      "Label distribution: [ 657 3656 2295 1696]\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data_path = os.path.join(project_root, \"data\", \"processed\", \"ProSeq_with_5component_analysis.csv\")\n",
    "sequences, labels = load_and_prepare_data_classification(data_path)\n",
    "print(f\"Loaded {len(sequences)} sequences with {len(set(labels))} unique classes\")\n",
    "print(f\"Label distribution: {np.bincount(labels)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "4aafca5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified faster CNN model\n",
    "\n",
    "class SimplifiedPromoterCNN(nn.Module):\n",
    "    def __init__(self, num_classes=4, max_length=400):  # Reduced from 600\n",
    "        super(SimplifiedPromoterCNN, self).__init__()\n",
    "        # Reduced number of channels and layers for speed\n",
    "        self.conv1 = nn.Conv1d(4, 16, kernel_size=8, padding=4)  # Smaller kernel, fewer channels\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=6, padding=3)  # Smaller kernel, fewer channels\n",
    "        \n",
    "        self.pool = nn.MaxPool1d(3)  # Larger pooling for more aggressive downsampling\n",
    "        self.dropout = nn.Dropout(0.2)  # Reduced dropout\n",
    "        \n",
    "        # Calculate the final feature size after convolutions and pooling\n",
    "        # After 2 conv+pool layers with pool size 3: max_length // (3^2) = max_length // 9\n",
    "        final_length = max_length // 9\n",
    "        self.fc1 = nn.Linear(32 * final_length, 64)  # Much smaller FC layer\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch, length, 4) -> transpose to (batch, 4, length) for Conv1d\n",
    "        if x.dim() == 3 and x.shape[-1] == 4:\n",
    "            x = x.transpose(1, 2)\n",
    "        \n",
    "        # First conv block\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Second conv block  \n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Flatten and fully connected layers\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "77fe5895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Total parameters: 94,068\n",
      "Trainable parameters: 94,068\n"
     ]
    }
   ],
   "source": [
    "# Get the best available device for training\n",
    "from src.utils.training import get_best_device\n",
    "\n",
    "device, loader_kwargs, device_name = get_best_device()\n",
    "print(f\"Using device: {device_name}\")\n",
    "\n",
    "# Create simplified model with reduced sequence length\n",
    "cnn = SimplifiedPromoterCNN(num_classes=4, max_length=400)\n",
    "cnn = cnn.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in cnn.parameters())\n",
    "trainable_params = sum(p.numel() for p in cnn.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "692dadf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaydenthai/Dev/University Work/2025sem2/Data Science Research Project /DS-Research-Project-Tumor-Expression/.conda/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Better optimizer and learning rate for faster convergence\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cnn.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Learning rate scheduler for adaptive learning\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "9678d096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and build loaders\n",
    "csv_path = \"../../data/processed/ProSeq_with_5component_analysis.csv\"\n",
    "sequences, targets = load_and_prepare_data_classification(csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "90a6b5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes - Train: 5314, Val: 1329, Test: 1661\n",
      "Sample data: {'sequence': tensor([[1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.]]), 'target': tensor(3)}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Use the classification dataset for integer labels\n",
    "\n",
    "train_seq, test_seq, train_labels, test_labels = train_test_split(\n",
    "    sequences, targets, test_size=0.2, random_state=42, stratify=targets\n",
    ")\n",
    "train_seq, val_seq, train_labels, val_labels = train_test_split(\n",
    "    train_seq, train_labels, test_size=0.2, random_state=4, stratify=train_labels\n",
    ")\n",
    "\n",
    "# Import the correct classification dataset\n",
    "from src.utils.data import PromoterClassificationDataset\n",
    "\n",
    "# Use reduced max_length for faster processing\n",
    "train_ds = PromoterClassificationDataset(train_seq, train_labels, max_length=400)\n",
    "val_ds = PromoterClassificationDataset(val_seq, val_labels, max_length=400)\n",
    "test_ds = PromoterClassificationDataset(test_seq, test_labels, max_length=400)\n",
    "\n",
    "# Increase batch size for better GPU utilization and faster training\n",
    "batch_size = 64  # Increased from 32\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, **loader_kwargs)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2, **loader_kwargs)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2, **loader_kwargs)\n",
    "\n",
    "print(f\"Dataset sizes - Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}\")\n",
    "print(\"Sample data:\", train_ds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "0a323fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch [ 1/100] Train Loss: 1.2937 | Val Loss: 1.2524 | Time: 26.0s | LR: 0.001000\n",
      "Epoch [ 2/100] Train Loss: 1.2417 | Val Loss: 1.2433 | Time: 26.0s | LR: 0.001000\n",
      "Epoch [ 3/100] Train Loss: 1.2424 | Val Loss: 1.2436 | Time: 26.0s | LR: 0.001000\n",
      "Epoch [ 4/100] Train Loss: 1.2484 | Val Loss: 1.2425 | Time: 25.9s | LR: 0.001000\n",
      "Epoch [ 5/100] Train Loss: 1.2387 | Val Loss: 1.2427 | Time: 26.3s | LR: 0.001000\n",
      "Epoch [ 6/100] Train Loss: 1.2452 | Val Loss: 1.2433 | Time: 26.2s | LR: 0.001000\n",
      "Epoch [ 7/100] Train Loss: 1.2499 | Val Loss: 1.2430 | Time: 26.2s | LR: 0.001000\n",
      "Epoch [ 8/100] Train Loss: 1.2448 | Val Loss: 1.2431 | Time: 26.2s | LR: 0.001000\n",
      "Epoch [ 9/100] Train Loss: 1.2463 | Val Loss: 1.2422 | Time: 26.0s | LR: 0.001000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[153]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m train_loss = train_epoch_ce(cnn, train_loader, criterion, optimizer, device)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Validation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m val_loss = \u001b[43mvalidate_epoch_ce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Learning rate scheduling\u001b[39;00m\n\u001b[32m     23\u001b[39m scheduler.step(val_loss)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/University Work/2025sem2/Data Science Research Project /DS-Research-Project-Tumor-Expression/src/utils/training.py:109\u001b[39m, in \u001b[36mvalidate_epoch_ce\u001b[39m\u001b[34m(model, val_loader, criterion, device)\u001b[39m\n\u001b[32m    107\u001b[39m total_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    111\u001b[39m \u001b[43m        \u001b[49m\u001b[43msequences\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msequence\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/University Work/2025sem2/Data Science Research Project /DS-Research-Project-Tumor-Expression/.conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/University Work/2025sem2/Data Science Research Project /DS-Research-Project-Tumor-Expression/.conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1437\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1434\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1435\u001b[39m     \u001b[38;5;66;03m# no valid `self._rcvd_idx` is found (i.e., didn't break)\u001b[39;00m\n\u001b[32m   1436\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._persistent_workers:\n\u001b[32m-> \u001b[39m\u001b[32m1437\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_shutdown_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1438\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[32m   1440\u001b[39m \u001b[38;5;66;03m# Now `self._rcvd_idx` is the batch index we want to fetch\u001b[39;00m\n\u001b[32m   1441\u001b[39m \n\u001b[32m   1442\u001b[39m \u001b[38;5;66;03m# Check if the next sample has already been generated\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/University Work/2025sem2/Data Science Research Project /DS-Research-Project-Tumor-Expression/.conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1568\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._shutdown_workers\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1563\u001b[39m         \u001b[38;5;28mself\u001b[39m._mark_worker_as_unavailable(worker_id, shutdown=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._workers:\n\u001b[32m   1565\u001b[39m     \u001b[38;5;66;03m# We should be able to join here, but in case anything went\u001b[39;00m\n\u001b[32m   1566\u001b[39m     \u001b[38;5;66;03m# wrong, we set a timeout and if the workers fail to join,\u001b[39;00m\n\u001b[32m   1567\u001b[39m     \u001b[38;5;66;03m# they are killed in the `finally` block.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1568\u001b[39m     \u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMP_STATUS_CHECK_INTERVAL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1569\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._index_queues:\n\u001b[32m   1570\u001b[39m     q.cancel_join_thread()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/University Work/2025sem2/Data Science Research Project /DS-Research-Project-Tumor-Expression/.conda/lib/python3.11/multiprocessing/process.py:149\u001b[39m, in \u001b[36mBaseProcess.join\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._parent_pid == os.getpid(), \u001b[33m'\u001b[39m\u001b[33mcan only join a child process\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._popen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m'\u001b[39m\u001b[33mcan only join a started process\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_popen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    151\u001b[39m     _children.discard(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/University Work/2025sem2/Data Science Research Project /DS-Research-Project-Tumor-Expression/.conda/lib/python3.11/multiprocessing/popen_fork.py:40\u001b[39m, in \u001b[36mPopen.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmultiprocessing\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconnection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wait\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msentinel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     41\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/University Work/2025sem2/Data Science Research Project /DS-Research-Project-Tumor-Expression/.conda/lib/python3.11/multiprocessing/connection.py:948\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(object_list, timeout)\u001b[39m\n\u001b[32m    945\u001b[39m     deadline = time.monotonic() + timeout\n\u001b[32m    947\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m948\u001b[39m     ready = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    949\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[32m    950\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [key.fileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/University Work/2025sem2/Data Science Research Project /DS-Research-Project-Tumor-Expression/.conda/lib/python3.11/selectors.py:415\u001b[39m, in \u001b[36m_PollLikeSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    413\u001b[39m ready = []\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     fd_event_list = \u001b[38;5;28mself\u001b[39m._selector.poll(timeout)\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Efficient training loop with validation and early stopping\n",
    "from src.utils.training import train_epoch_ce, validate_epoch_ce\n",
    "import time\n",
    "\n",
    "num_epochs = 100  # Reduced epochs with early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 100\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"Starting training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Training\n",
    "    train_loss = train_epoch_ce(cnn, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validation\n",
    "    val_loss = validate_epoch_ce(cnn, val_loader, criterion, device)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save(cnn.state_dict(), 'best_simplified_cnn.pt')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    print(f'Epoch [{epoch+1:2d}/{num_epochs}] '\n",
    "          f'Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | '\n",
    "          f'Time: {epoch_time:.1f}s | LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f'\\nTraining completed in {total_time:.1f} seconds')\n",
    "print(f'Best validation loss: {best_val_loss:.4f}')\n",
    "\n",
    "# Load best model for evaluation\n",
    "cnn.load_state_dict(torch.load('best_simplified_cnn.pt'))\n",
    "print(\"Loaded best model weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f198a979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "9e82ad70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.2383153438568115, Val loss: 1.219427227973938\n",
      "Train loss: 1.2417333126068115, Val loss: 1.2192087173461914\n",
      "Train loss: 1.2182636260986328, Val loss: 1.2189810276031494\n",
      "Train loss: 1.1986217498779297, Val loss: 1.218729853630066\n",
      "Train loss: 1.2234951257705688, Val loss: 1.2184820175170898\n",
      "Train loss: 1.217616319656372, Val loss: 1.2182201147079468\n",
      "Train loss: 1.197298288345337, Val loss: 1.2179462909698486\n",
      "Train loss: 1.211430549621582, Val loss: 1.2176754474639893\n",
      "Train loss: 1.2295634746551514, Val loss: 1.2174067497253418\n",
      "Train loss: 1.2208251953125, Val loss: 1.2171376943588257\n",
      "Train loss: 1.2183176279067993, Val loss: 1.2168627977371216\n",
      "Train loss: 1.2358319759368896, Val loss: 1.2165920734405518\n",
      "Train loss: 1.23792564868927, Val loss: 1.2163264751434326\n",
      "Train loss: 1.2487882375717163, Val loss: 1.2160704135894775\n",
      "Train loss: 1.2250200510025024, Val loss: 1.215811848640442\n",
      "Train loss: 1.2198175191879272, Val loss: 1.2155628204345703\n",
      "Train loss: 1.2298556566238403, Val loss: 1.2153172492980957\n",
      "Train loss: 1.2305084466934204, Val loss: 1.2150709629058838\n",
      "Train loss: 1.2004508972167969, Val loss: 1.2148091793060303\n",
      "Train loss: 1.2225130796432495, Val loss: 1.214546799659729\n",
      "Train loss: 1.230187177658081, Val loss: 1.2142854928970337\n",
      "Train loss: 1.2123498916625977, Val loss: 1.2140276432037354\n",
      "Train loss: 1.2166821956634521, Val loss: 1.2137645483016968\n",
      "Train loss: 1.2214239835739136, Val loss: 1.213498592376709\n",
      "Train loss: 1.2328269481658936, Val loss: 1.213235855102539\n",
      "Train loss: 1.2214179039001465, Val loss: 1.2129707336425781\n",
      "Train loss: 1.2136484384536743, Val loss: 1.2127060890197754\n",
      "Train loss: 1.2324779033660889, Val loss: 1.2124338150024414\n",
      "Train loss: 1.2332582473754883, Val loss: 1.2121641635894775\n",
      "Train loss: 1.2071540355682373, Val loss: 1.2118773460388184\n",
      "Train loss: 1.2229359149932861, Val loss: 1.2115896940231323\n",
      "Train loss: 1.2320241928100586, Val loss: 1.2112936973571777\n",
      "Train loss: 1.20550537109375, Val loss: 1.2109804153442383\n",
      "Train loss: 1.2213612794876099, Val loss: 1.2106618881225586\n",
      "Train loss: 1.2157351970672607, Val loss: 1.2103476524353027\n",
      "Train loss: 1.2080121040344238, Val loss: 1.210031509399414\n",
      "Train loss: 1.190795660018921, Val loss: 1.209704875946045\n",
      "Train loss: 1.2072970867156982, Val loss: 1.2093923091888428\n",
      "Train loss: 1.2176494598388672, Val loss: 1.209080696105957\n",
      "Train loss: 1.2164673805236816, Val loss: 1.208765983581543\n",
      "Train loss: 1.2069132328033447, Val loss: 1.208443284034729\n",
      "Train loss: 1.2037591934204102, Val loss: 1.208106279373169\n",
      "Train loss: 1.215267539024353, Val loss: 1.2077757120132446\n",
      "Train loss: 1.2155513763427734, Val loss: 1.2074451446533203\n",
      "Train loss: 1.2127060890197754, Val loss: 1.2071161270141602\n",
      "Train loss: 1.207391619682312, Val loss: 1.2067828178405762\n",
      "Train loss: 1.2321436405181885, Val loss: 1.2064590454101562\n",
      "Train loss: 1.2005627155303955, Val loss: 1.2061278820037842\n",
      "Train loss: 1.2195219993591309, Val loss: 1.2057790756225586\n",
      "Train loss: 1.2239916324615479, Val loss: 1.2054226398468018\n",
      "Train loss: 1.2109508514404297, Val loss: 1.205049753189087\n",
      "Train loss: 1.2033119201660156, Val loss: 1.2046828269958496\n",
      "Train loss: 1.213687777519226, Val loss: 1.2043421268463135\n",
      "Train loss: 1.214339017868042, Val loss: 1.2039999961853027\n",
      "Train loss: 1.2180695533752441, Val loss: 1.2036583423614502\n",
      "Train loss: 1.2051111459732056, Val loss: 1.203315258026123\n",
      "Train loss: 1.1932272911071777, Val loss: 1.2029621601104736\n",
      "Train loss: 1.2197332382202148, Val loss: 1.2026145458221436\n",
      "Train loss: 1.2147810459136963, Val loss: 1.2022625207901\n",
      "Train loss: 1.2085555791854858, Val loss: 1.2019062042236328\n",
      "Train loss: 1.181739091873169, Val loss: 1.2015345096588135\n",
      "Train loss: 1.170540690422058, Val loss: 1.2011387348175049\n",
      "Train loss: 1.2111650705337524, Val loss: 1.200737714767456\n",
      "Train loss: 1.2347115278244019, Val loss: 1.2003555297851562\n",
      "Train loss: 1.1973645687103271, Val loss: 1.1999595165252686\n",
      "Train loss: 1.2096779346466064, Val loss: 1.199568748474121\n",
      "Train loss: 1.2168830633163452, Val loss: 1.1991851329803467\n",
      "Train loss: 1.2083137035369873, Val loss: 1.198815107345581\n",
      "Train loss: 1.189056158065796, Val loss: 1.1984378099441528\n",
      "Train loss: 1.205658197402954, Val loss: 1.1980557441711426\n",
      "Train loss: 1.1893953084945679, Val loss: 1.1976666450500488\n",
      "Train loss: 1.2037076950073242, Val loss: 1.1972770690917969\n",
      "Train loss: 1.2136344909667969, Val loss: 1.1968979835510254\n",
      "Train loss: 1.2022521495819092, Val loss: 1.1965179443359375\n",
      "Train loss: 1.2063231468200684, Val loss: 1.1961416006088257\n",
      "Train loss: 1.2035564184188843, Val loss: 1.1957612037658691\n",
      "Train loss: 1.1966193914413452, Val loss: 1.195376992225647\n",
      "Train loss: 1.198060393333435, Val loss: 1.1949951648712158\n",
      "Train loss: 1.2110264301300049, Val loss: 1.1946206092834473\n",
      "Train loss: 1.2249348163604736, Val loss: 1.194256067276001\n",
      "Train loss: 1.1883503198623657, Val loss: 1.1938912868499756\n",
      "Train loss: 1.2056492567062378, Val loss: 1.1935328245162964\n",
      "Train loss: 1.2035609483718872, Val loss: 1.1931736469268799\n",
      "Train loss: 1.1923372745513916, Val loss: 1.192806363105774\n",
      "Train loss: 1.2018495798110962, Val loss: 1.1924455165863037\n",
      "Train loss: 1.2075759172439575, Val loss: 1.1920878887176514\n",
      "Train loss: 1.2160589694976807, Val loss: 1.1917327642440796\n",
      "Train loss: 1.1923819780349731, Val loss: 1.1913636922836304\n",
      "Train loss: 1.1875584125518799, Val loss: 1.1909780502319336\n",
      "Train loss: 1.1848742961883545, Val loss: 1.190589189529419\n",
      "Train loss: 1.1928801536560059, Val loss: 1.190197467803955\n",
      "Train loss: 1.1916332244873047, Val loss: 1.1897852420806885\n",
      "Train loss: 1.2093881368637085, Val loss: 1.1893692016601562\n",
      "Train loss: 1.1873046159744263, Val loss: 1.188942790031433\n",
      "Train loss: 1.2122447490692139, Val loss: 1.1885333061218262\n",
      "Train loss: 1.1856075525283813, Val loss: 1.1881186962127686\n",
      "Train loss: 1.202368974685669, Val loss: 1.187708854675293\n",
      "Train loss: 1.1911567449569702, Val loss: 1.1872864961624146\n",
      "Train loss: 1.200182557106018, Val loss: 1.186866283416748\n",
      "Train loss: 1.1904270648956299, Val loss: 1.1864382028579712\n",
      "Train loss: 1.1881908178329468, Val loss: 1.1860084533691406\n",
      "Train loss: 1.1906206607818604, Val loss: 1.1855658292770386\n",
      "Train loss: 1.1954448223114014, Val loss: 1.1851083040237427\n",
      "Train loss: 1.1610970497131348, Val loss: 1.1846250295639038\n",
      "Train loss: 1.1832427978515625, Val loss: 1.184138298034668\n",
      "Train loss: 1.200728178024292, Val loss: 1.1836590766906738\n",
      "Train loss: 1.1951184272766113, Val loss: 1.1831862926483154\n",
      "Train loss: 1.1917762756347656, Val loss: 1.1827116012573242\n",
      "Train loss: 1.171679973602295, Val loss: 1.18223237991333\n",
      "Train loss: 1.2106150388717651, Val loss: 1.181769847869873\n",
      "Train loss: 1.1972317695617676, Val loss: 1.1813082695007324\n",
      "Train loss: 1.1728957891464233, Val loss: 1.1808390617370605\n",
      "Train loss: 1.17698073387146, Val loss: 1.180370569229126\n",
      "Train loss: 1.1998860836029053, Val loss: 1.1799029111862183\n",
      "Train loss: 1.1919300556182861, Val loss: 1.179443120956421\n",
      "Train loss: 1.1956684589385986, Val loss: 1.178985595703125\n",
      "Train loss: 1.1894162893295288, Val loss: 1.178520679473877\n",
      "Train loss: 1.1909868717193604, Val loss: 1.1780492067337036\n",
      "Train loss: 1.1853101253509521, Val loss: 1.1775742769241333\n",
      "Train loss: 1.2179780006408691, Val loss: 1.1771211624145508\n",
      "Train loss: 1.1871788501739502, Val loss: 1.1766771078109741\n",
      "Train loss: 1.1804373264312744, Val loss: 1.1762268543243408\n",
      "Train loss: 1.2021982669830322, Val loss: 1.175781488418579\n",
      "Train loss: 1.1819281578063965, Val loss: 1.175321102142334\n",
      "Train loss: 1.1848527193069458, Val loss: 1.1748607158660889\n",
      "Train loss: 1.207617998123169, Val loss: 1.1744000911712646\n",
      "Train loss: 1.1993772983551025, Val loss: 1.1739473342895508\n",
      "Train loss: 1.1977580785751343, Val loss: 1.1734960079193115\n",
      "Train loss: 1.2040019035339355, Val loss: 1.1730437278747559\n",
      "Train loss: 1.1871886253356934, Val loss: 1.1725847721099854\n",
      "Train loss: 1.1708807945251465, Val loss: 1.1721100807189941\n",
      "Train loss: 1.200728416442871, Val loss: 1.1716400384902954\n",
      "Train loss: 1.1896240711212158, Val loss: 1.1711772680282593\n",
      "Train loss: 1.1578447818756104, Val loss: 1.1706933975219727\n",
      "Train loss: 1.1801071166992188, Val loss: 1.170219898223877\n",
      "Train loss: 1.14608895778656, Val loss: 1.1697267293930054\n",
      "Train loss: 1.1834560632705688, Val loss: 1.1692174673080444\n",
      "Train loss: 1.1655094623565674, Val loss: 1.1686933040618896\n",
      "Train loss: 1.1963130235671997, Val loss: 1.1681780815124512\n",
      "Train loss: 1.1689937114715576, Val loss: 1.1676537990570068\n",
      "Train loss: 1.1797584295272827, Val loss: 1.1671221256256104\n",
      "Train loss: 1.163271427154541, Val loss: 1.1665722131729126\n",
      "Train loss: 1.1657860279083252, Val loss: 1.1660192012786865\n",
      "Train loss: 1.1754703521728516, Val loss: 1.1654685735702515\n",
      "Train loss: 1.1683191061019897, Val loss: 1.1649115085601807\n",
      "Train loss: 1.1939113140106201, Val loss: 1.1643602848052979\n",
      "Train loss: 1.1603201627731323, Val loss: 1.1637879610061646\n",
      "Train loss: 1.2006776332855225, Val loss: 1.1632122993469238\n",
      "Train loss: 1.1499683856964111, Val loss: 1.1626216173171997\n",
      "Train loss: 1.1568454504013062, Val loss: 1.1620197296142578\n",
      "Train loss: 1.1756505966186523, Val loss: 1.16141676902771\n",
      "Train loss: 1.1835179328918457, Val loss: 1.1608185768127441\n",
      "Train loss: 1.196164608001709, Val loss: 1.1602386236190796\n",
      "Train loss: 1.183154821395874, Val loss: 1.159665584564209\n",
      "Train loss: 1.1752433776855469, Val loss: 1.1590957641601562\n",
      "Train loss: 1.166446566581726, Val loss: 1.1585224866867065\n",
      "Train loss: 1.1547770500183105, Val loss: 1.15793776512146\n",
      "Train loss: 1.1679751873016357, Val loss: 1.1573566198349\n",
      "Train loss: 1.1742737293243408, Val loss: 1.1567785739898682\n",
      "Train loss: 1.1563646793365479, Val loss: 1.1561830043792725\n",
      "Train loss: 1.1765191555023193, Val loss: 1.1555914878845215\n",
      "Train loss: 1.1363928318023682, Val loss: 1.1549782752990723\n",
      "Train loss: 1.187084436416626, Val loss: 1.1543735265731812\n",
      "Train loss: 1.1661219596862793, Val loss: 1.153773307800293\n",
      "Train loss: 1.1805832386016846, Val loss: 1.1531699895858765\n",
      "Train loss: 1.174417495727539, Val loss: 1.1525706052780151\n",
      "Train loss: 1.1667327880859375, Val loss: 1.1519724130630493\n",
      "Train loss: 1.1634724140167236, Val loss: 1.1513705253601074\n",
      "Train loss: 1.1725355386734009, Val loss: 1.1507582664489746\n",
      "Train loss: 1.163170337677002, Val loss: 1.150156021118164\n",
      "Train loss: 1.1667333841323853, Val loss: 1.1495592594146729\n",
      "Train loss: 1.148566722869873, Val loss: 1.1489439010620117\n",
      "Train loss: 1.1522297859191895, Val loss: 1.1483205556869507\n",
      "Train loss: 1.1633646488189697, Val loss: 1.1476951837539673\n",
      "Train loss: 1.1377601623535156, Val loss: 1.1470563411712646\n",
      "Train loss: 1.156665563583374, Val loss: 1.1464028358459473\n",
      "Train loss: 1.1597113609313965, Val loss: 1.1457328796386719\n",
      "Train loss: 1.1631622314453125, Val loss: 1.1450576782226562\n",
      "Train loss: 1.163161039352417, Val loss: 1.1443697214126587\n",
      "Train loss: 1.163494348526001, Val loss: 1.143676519393921\n",
      "Train loss: 1.1506857872009277, Val loss: 1.1429780721664429\n",
      "Train loss: 1.1210901737213135, Val loss: 1.1422697305679321\n",
      "Train loss: 1.173255443572998, Val loss: 1.141573429107666\n",
      "Train loss: 1.139573097229004, Val loss: 1.140857219696045\n",
      "Train loss: 1.176754117012024, Val loss: 1.1401535272598267\n",
      "Train loss: 1.1375360488891602, Val loss: 1.139436960220337\n",
      "Train loss: 1.165770173072815, Val loss: 1.138725757598877\n",
      "Train loss: 1.1505317687988281, Val loss: 1.1379985809326172\n",
      "Train loss: 1.129227638244629, Val loss: 1.1372511386871338\n",
      "Train loss: 1.1544036865234375, Val loss: 1.1365076303482056\n",
      "Train loss: 1.1221598386764526, Val loss: 1.1357581615447998\n",
      "Train loss: 1.1887481212615967, Val loss: 1.135019063949585\n",
      "Train loss: 1.122178077697754, Val loss: 1.1342639923095703\n",
      "Train loss: 1.1441097259521484, Val loss: 1.1334989070892334\n",
      "Train loss: 1.1634557247161865, Val loss: 1.1327505111694336\n",
      "Train loss: 1.1605942249298096, Val loss: 1.1319938898086548\n",
      "Train loss: 1.120568037033081, Val loss: 1.131209373474121\n",
      "Train loss: 1.143020510673523, Val loss: 1.1304023265838623\n",
      "Train loss: 1.1614984273910522, Val loss: 1.129594087600708\n",
      "Train loss: 1.1729421615600586, Val loss: 1.1288118362426758\n"
     ]
    }
   ],
   "source": [
    "subset = torch.utils.data.Subset(train_ds, range(32))\n",
    "tiny_loader = DataLoader(subset, batch_size=32, shuffle=True)\n",
    "for _ in range(200):\n",
    "    train = train_epoch_ce(cnn, tiny_loader, criterion, optimizer, device)  # expect loss to plummet\n",
    "    val = validate_epoch_ce(cnn, tiny_loader, criterion, device)\n",
    "\n",
    "    print(f\"Train loss: {train}, Val loss: {val}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a16aaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the simplified model\n",
    "from src.utils.training import evaluate_model_ce\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Get predictions on test set\n",
    "test_logits, test_labels = evaluate_model_ce(cnn, test_loader, device)\n",
    "test_predictions = np.argmax(test_logits, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_labels, test_predictions, target_names=[f'Class {i}' for i in range(4)]))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(test_labels, test_predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=[f'Class {i}' for i in range(4)],\n",
    "            yticklabels=[f'Class {i}' for i in range(4)])\n",
    "plt.title('Confusion Matrix - Simplified CNN')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../plots/simplified_cnn_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
