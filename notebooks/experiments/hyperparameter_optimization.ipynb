{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyperparameter Optimization for Promoter CNN\n",
        "\n",
        "This notebook contains comprehensive hyperparameter optimization for the Promoter CNN model using multiple search strategies:\n",
        "- Random Search\n",
        "- Grid Search  \n",
        "- Bayesian Optimization\n",
        "- Cross-Validation\n",
        "\n",
        "The optimized hyperparameters can then be used in the main promoter_cnn.ipynb notebook for final training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.5.1\n",
            "CUDA available: False\n",
            "MPS available: True\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Required imports for hyperparameter optimization\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Tuple, Dict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import tuning specific libraries\n",
        "import random\n",
        "import time\n",
        "import itertools\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, Any, List, Optional, Tuple\n",
        "import json\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"MPS available: {getattr(torch.backends, 'mps', None) is not None and torch.backends.mps.is_available()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset and Model Classes\n",
        "\n",
        "Import the dataset class and model architecture from the main implementation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PromoterDataset(Dataset):\n",
        "    \"\"\"Dataset for promoter sequences only\"\"\"\n",
        "    \n",
        "    def __init__(self, sequences: list, targets: np.ndarray):\n",
        "        self.sequences = sequences\n",
        "        self.targets = targets\n",
        "        \n",
        "        # DNA encoding dictionary\n",
        "        self.dna_dict = {'A': 0, 'T': 1, 'G': 2, 'C': 3, 'N': 4}\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "    \n",
        "    def encode_sequence(self, sequence: str, max_length: int = 600) -> np.ndarray:\n",
        "        \"\"\"Encode DNA sequence to numerical representation\"\"\"\n",
        "        # Truncate or pad sequence\n",
        "        if len(sequence) > max_length:\n",
        "            sequence = sequence[:max_length]\n",
        "        else:\n",
        "            sequence = sequence + 'N' * (max_length - len(sequence))\n",
        "        \n",
        "        # Convert to numerical encoding\n",
        "        encoded = np.array([self.dna_dict.get(base.upper(), 4) for base in sequence])\n",
        "        \n",
        "        # One-hot encode\n",
        "        one_hot = np.zeros((max_length, 5))\n",
        "        one_hot[np.arange(max_length), encoded] = 1\n",
        "        \n",
        "        return one_hot.T  # Shape: (5, max_length) for Conv1d\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        sequence = self.encode_sequence(self.sequences[idx])\n",
        "        target = self.targets[idx].astype(np.float32)\n",
        "        total = float(np.sum(target))\n",
        "        if total <= 0:\n",
        "            target = np.ones_like(target, dtype=np.float32) / target.shape[0]\n",
        "        else:\n",
        "            target = target / total\n",
        "        \n",
        "        return {\n",
        "            'sequence': torch.FloatTensor(sequence),\n",
        "            'target': torch.FloatTensor(target)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class PromoterCNN(nn.Module):\n",
        "    \"\"\"CNN for predicting 5-component probabilities with configurable depth.\"\"\"\n",
        "    \n",
        "    def __init__(self, sequence_length: int = 600, num_blocks: int = 2, base_channels: int = 64, dropout: float = 0.3):\n",
        "        super(PromoterCNN, self).__init__()\n",
        "        assert num_blocks >= 1\n",
        "        \n",
        "        conv_layers = []\n",
        "        in_ch = 5\n",
        "        out_ch = base_channels\n",
        "        for i in range(num_blocks):\n",
        "            k = 11 if i == 0 else 7\n",
        "            p = 5 if i == 0 else 3\n",
        "            conv_layers.append(nn.Conv1d(in_channels=in_ch, out_channels=out_ch, kernel_size=k, padding=p))\n",
        "            conv_layers.append(nn.ReLU())\n",
        "            if i < min(2, num_blocks):\n",
        "                conv_layers.append(nn.MaxPool1d(kernel_size=4))\n",
        "            conv_layers.append(nn.Dropout(dropout))\n",
        "            in_ch = out_ch\n",
        "            out_ch = min(out_ch * 2, base_channels * (2 ** (num_blocks - 1)))\n",
        "        conv_layers.append(nn.AdaptiveAvgPool1d(1))\n",
        "        self.sequence_conv = nn.Sequential(*conv_layers)\n",
        "        \n",
        "        final_ch = in_ch\n",
        "        hidden = max(32, final_ch // 2)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(final_ch, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden, 5)  # 5 components\n",
        "        )\n",
        "        \n",
        "    def forward(self, sequence):\n",
        "        x = self.sequence_conv(sequence)\n",
        "        x = x.squeeze(-1)\n",
        "        logits = self.classifier(x)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading and Training Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_and_prepare_data(file_path: str) -> Tuple[list, np.ndarray]:\n",
        "    \"\"\"Load and prepare data for training\"\"\"\n",
        "    print(\"Loading data...\")\n",
        "    df = pd.read_csv(file_path)\n",
        "    \n",
        "    # Component probabilities as targets\n",
        "    prob_cols = ['Component_1_Probability', 'Component_2_Probability', \n",
        "                'Component_3_Probability', 'Component_4_Probability', 'Component_5_Probability']\n",
        "    \n",
        "    # Filter out rows with missing sequence data or target data\n",
        "    print(f\"Initial data shape: {df.shape}\")\n",
        "    \n",
        "    # Remove rows with NaN in ProSeq column\n",
        "    df = df.dropna(subset=['ProSeq'])\n",
        "    print(f\"After removing missing sequences: {df.shape}\")\n",
        "    \n",
        "    # Remove rows with NaN in any probability column\n",
        "    df = df.dropna(subset=prob_cols)\n",
        "    print(f\"After removing missing probabilities: {df.shape}\")\n",
        "    \n",
        "    # Extract features and targets\n",
        "    sequences = df['ProSeq'].tolist()\n",
        "    targets = df[prob_cols].values\n",
        "    \n",
        "    # Additional validation - ensure all sequences are strings\n",
        "    valid_sequences = []\n",
        "    valid_targets = []\n",
        "    \n",
        "    for i, seq in enumerate(sequences):\n",
        "        if isinstance(seq, str) and len(seq) > 0:\n",
        "            valid_sequences.append(seq)\n",
        "            valid_targets.append(targets[i])\n",
        "    \n",
        "    sequences = valid_sequences\n",
        "    targets = np.array(valid_targets)\n",
        "    \n",
        "    print(f\"Final dataset: {len(sequences)} samples\")\n",
        "    print(f\"Average sequence length: {np.mean([len(seq) for seq in sequences]):.1f}\")\n",
        "    print(f\"Min sequence length: {min([len(seq) for seq in sequences])}\")\n",
        "    print(f\"Max sequence length: {max([len(seq) for seq in sequences])}\")\n",
        "    \n",
        "    return sequences, targets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    \n",
        "    for batch in train_loader:\n",
        "        sequences = batch['sequence'].to(device)\n",
        "        targets = batch['target'].to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        logits = model(sequences)\n",
        "        log_probs = F.log_softmax(logits, dim=1)\n",
        "        loss = criterion(log_probs, targets)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "    \n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def validate_epoch(model, val_loader, criterion, device):\n",
        "    \"\"\"Validate for one epoch\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            sequences = batch['sequence'].to(device)\n",
        "            targets = batch['target'].to(device)\n",
        "            \n",
        "            logits = model(sequences)\n",
        "            log_probs = F.log_softmax(logits, dim=1)\n",
        "            loss = criterion(log_probs, targets)\n",
        "            total_loss += loss.item()\n",
        "    \n",
        "    return total_loss / len(val_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameter Tuning Framework\n",
        "\n",
        "The comprehensive hyperparameter optimization framework with multiple search strategies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class TrialResult:\n",
        "    config: Dict[str, Any]\n",
        "    val_loss: float\n",
        "    val_losses: List[float] = field(default_factory=list)\n",
        "    train_losses: List[float] = field(default_factory=list)\n",
        "    params: int = 0\n",
        "    duration_s: float = 0.0\n",
        "    epochs_trained: int = 0\n",
        "    cv_scores: List[float] = field(default_factory=list)\n",
        "    cv_mean: float = 0.0\n",
        "    cv_std: float = 0.0\n",
        "    final_lr: float = 0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HyperparameterTuner:\n",
        "    \"\"\"Comprehensive hyperparameter tuning framework with multiple search strategies\"\"\"\n",
        "    \n",
        "    def __init__(self, train_dataset, val_dataset, device, save_dir=\"tuning_results\"):\n",
        "        self.train_dataset = train_dataset\n",
        "        self.val_dataset = val_dataset\n",
        "        self.device = device\n",
        "        self.save_dir = save_dir\n",
        "        self.results = []\n",
        "        \n",
        "        # Extensive search space\n",
        "        self.SEARCH_SPACE = {\n",
        "            # Architecture parameters\n",
        "            'depth': [1, 2, 3, 4],\n",
        "            'base_channels': [8, 16, 24, 32, 48, 64],\n",
        "            'dropout': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "            \n",
        "            # Optimizer parameters\n",
        "            'optimizer': ['adam', 'adamw', 'sgd', 'rmsprop'],\n",
        "            'lr': [1e-4, 3e-4, 1e-3, 3e-3, 1e-2],\n",
        "            'weight_decay': [0, 1e-6, 1e-5, 1e-4, 3e-4, 1e-3],\n",
        "            \n",
        "            # Training parameters\n",
        "            'batch_size': [16, 32, 64, 128],\n",
        "            'scheduler': ['plateau', 'cosine', 'step', 'none'],\n",
        "            \n",
        "            # SGD specific\n",
        "            'momentum': [0.9, 0.95, 0.99],\n",
        "            'nesterov': [True, False],\n",
        "            \n",
        "            # Scheduler specific\n",
        "            'scheduler_patience': [3, 5, 8, 10],\n",
        "            'scheduler_factor': [0.1, 0.3, 0.5, 0.7],\n",
        "            'step_size': [10, 20, 30],\n",
        "            'gamma': [0.1, 0.3, 0.5],\n",
        "            \n",
        "            # Loss function\n",
        "            'loss_function': ['kldiv', 'mse', 'smooth_l1', 'cross_entropy']\n",
        "        }\n",
        "    \n",
        "    def build_loaders(self, batch_size: int):\n",
        "        \"\"\"Build data loaders with specified batch size\"\"\"\n",
        "        train_loader = DataLoader(self.train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "        val_loader = DataLoader(self.val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "        return train_loader, val_loader\n",
        "    \n",
        "    def create_optimizer(self, model, config):\n",
        "        \"\"\"Create optimizer based on configuration\"\"\"\n",
        "        params = model.parameters()\n",
        "        lr = config['lr']\n",
        "        wd = config['weight_decay']\n",
        "        \n",
        "        if config['optimizer'] == 'adam':\n",
        "            return optim.Adam(params, lr=lr, weight_decay=wd)\n",
        "        elif config['optimizer'] == 'adamw':\n",
        "            return optim.AdamW(params, lr=lr, weight_decay=wd)\n",
        "        elif config['optimizer'] == 'sgd':\n",
        "            momentum = config.get('momentum', 0.9)\n",
        "            nesterov = config.get('nesterov', True)\n",
        "            return optim.SGD(params, lr=lr, weight_decay=wd, momentum=momentum, nesterov=nesterov)\n",
        "        elif config['optimizer'] == 'rmsprop':\n",
        "            return optim.RMSprop(params, lr=lr, weight_decay=wd)\n",
        "        else:\n",
        "            return optim.Adam(params, lr=lr, weight_decay=wd)\n",
        "    \n",
        "    def create_scheduler(self, optimizer, config):\n",
        "        \"\"\"Create learning rate scheduler based on configuration\"\"\"\n",
        "        scheduler_type = config.get('scheduler', 'plateau')\n",
        "        \n",
        "        if scheduler_type == 'plateau':\n",
        "            patience = config.get('scheduler_patience', 5)\n",
        "            factor = config.get('scheduler_factor', 0.5)\n",
        "            return optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=patience, factor=factor)\n",
        "        elif scheduler_type == 'cosine':\n",
        "            return optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
        "        elif scheduler_type == 'step':\n",
        "            step_size = config.get('step_size', 20)\n",
        "            gamma = config.get('gamma', 0.5)\n",
        "            return optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "        else:\n",
        "            return None\n",
        "    \n",
        "    def create_criterion(self, config):\n",
        "        \"\"\"Create loss function based on configuration\"\"\"\n",
        "        loss_type = config.get('loss_function', 'kldiv')\n",
        "        \n",
        "        if loss_type == 'kldiv':\n",
        "            return nn.KLDivLoss(reduction='batchmean')\n",
        "        elif loss_type == 'mse':\n",
        "            return nn.MSELoss()\n",
        "        elif loss_type == 'smooth_l1':\n",
        "            return nn.SmoothL1Loss()\n",
        "        elif loss_type == 'cross_entropy':\n",
        "            return nn.CrossEntropyLoss()\n",
        "        else:\n",
        "            return nn.KLDivLoss(reduction='batchmean')\n",
        "    \n",
        "    def run_trial(self, config: Dict[str, Any], max_epochs: int = 25, es_patience: int = 8, verbose: bool = False) -> TrialResult:\n",
        "        \"\"\"Run a single hyperparameter trial\"\"\"\n",
        "        try:\n",
        "            # Build data loaders\n",
        "            train_loader, val_loader = self.build_loaders(config['batch_size'])\n",
        "            \n",
        "            # Create model\n",
        "            model = PromoterCNN(\n",
        "                num_blocks=config['depth'],\n",
        "                base_channels=config['base_channels'],\n",
        "                dropout=config['dropout']\n",
        "            ).to(self.device)\n",
        "            \n",
        "            param_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "            \n",
        "            # Create optimizer, scheduler, and criterion\n",
        "            optimizer = self.create_optimizer(model, config)\n",
        "            scheduler = self.create_scheduler(optimizer, config)\n",
        "            criterion = self.create_criterion(config)\n",
        "            \n",
        "            # Training loop\n",
        "            best_val_loss = float('inf')\n",
        "            bad_epochs = 0\n",
        "            train_losses = []\n",
        "            val_losses = []\n",
        "            start_time = time.time()\n",
        "            \n",
        "            for epoch in range(max_epochs):\n",
        "                # Train\n",
        "                train_loss = train_epoch(model, train_loader, criterion, optimizer, self.device)\n",
        "                \n",
        "                # Validate\n",
        "                val_loss = validate_epoch(model, val_loader, criterion, self.device)\n",
        "                \n",
        "                # Update scheduler\n",
        "                if scheduler is not None:\n",
        "                    if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
        "                        scheduler.step(val_loss)\n",
        "                    else:\n",
        "                        scheduler.step()\n",
        "                \n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                \n",
        "                # Early stopping\n",
        "                if val_loss < best_val_loss - 1e-6:\n",
        "                    best_val_loss = val_loss\n",
        "                    bad_epochs = 0\n",
        "                else:\n",
        "                    bad_epochs += 1\n",
        "                    if bad_epochs >= es_patience:\n",
        "                        if verbose:\n",
        "                            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                        break\n",
        "                \n",
        "                if verbose and epoch % 5 == 0:\n",
        "                    current_lr = optimizer.param_groups[0]['lr']\n",
        "                    print(f\"Epoch {epoch+1}: train={train_loss:.6f}, val={val_loss:.6f}, lr={current_lr:.2e}\")\n",
        "            \n",
        "            duration = time.time() - start_time\n",
        "            final_lr = optimizer.param_groups[0]['lr']\n",
        "            \n",
        "            return TrialResult(\n",
        "                config=config,\n",
        "                val_loss=best_val_loss,\n",
        "                val_losses=val_losses,\n",
        "                train_losses=train_losses,\n",
        "                params=param_count,\n",
        "                duration_s=duration,\n",
        "                epochs_trained=len(train_losses),\n",
        "                final_lr=final_lr\n",
        "            )\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Trial failed with config {config}: {e}\")\n",
        "            return TrialResult(\n",
        "                config=config,\n",
        "                val_loss=float('inf'),\n",
        "                params=0,\n",
        "                duration_s=0.0\n",
        "            )\n",
        "    \n",
        "    def random_search(self, num_trials: int = 50, seed: int = 42) -> List[TrialResult]:\n",
        "        \"\"\"Perform random search over hyperparameter space\"\"\"\n",
        "        print(f\"ðŸ” Starting Random Search with {num_trials} trials...\")\n",
        "        random.seed(seed)\n",
        "        results = []\n",
        "        \n",
        "        for trial in range(1, num_trials + 1):\n",
        "            # Sample configuration\n",
        "            config = {key: random.choice(values) for key, values in self.SEARCH_SPACE.items()}\n",
        "            \n",
        "            # Clean up SGD-specific parameters if not using SGD\n",
        "            if config['optimizer'] != 'sgd':\n",
        "                config.pop('momentum', None)\n",
        "                config.pop('nesterov', None)\n",
        "            \n",
        "            # Clean up scheduler-specific parameters\n",
        "            scheduler_type = config.get('scheduler', 'plateau')\n",
        "            if scheduler_type != 'plateau':\n",
        "                config.pop('scheduler_patience', None)\n",
        "                config.pop('scheduler_factor', None)\n",
        "            if scheduler_type != 'step':\n",
        "                config.pop('step_size', None)\n",
        "                config.pop('gamma', None)\n",
        "            \n",
        "            # Run trial\n",
        "            result = self.run_trial(config)\n",
        "            results.append(result)\n",
        "            \n",
        "            print(f\"Trial {trial:3d}/{num_trials}: val_loss={result.val_loss:.6f}, \"\n",
        "                  f\"params={result.params:,}, time={result.duration_s:.1f}s\")\n",
        "            \n",
        "            # Print best config so far\n",
        "            if trial % 10 == 0:\n",
        "                best_so_far = min(results, key=lambda r: r.val_loss)\n",
        "                print(f\"   Best so far: {best_so_far.val_loss:.6f} with {best_so_far.params:,} params\")\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def save_results(self, results: List[TrialResult], filename: str = None):\n",
        "        \"\"\"Save tuning results to file\"\"\"\n",
        "        if filename is None:\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            filename = f\"tuning_results_{timestamp}.pkl\"\n",
        "        \n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump(results, f)\n",
        "        print(f\"ðŸ’¾ Results saved to {filename}\")\n",
        "    \n",
        "    def analyze_results(self, results: List[TrialResult], top_k: int = 5):\n",
        "        \"\"\"Analyze tuning results\"\"\"\n",
        "        if not results:\n",
        "            print(\"No results to analyze!\")\n",
        "            return\n",
        "        \n",
        "        # Sort by validation loss\n",
        "        results_sorted = sorted(results, key=lambda r: r.val_loss if r.val_loss != float('inf') else 999)\n",
        "        valid_results = [r for r in results_sorted if r.val_loss != float('inf')]\n",
        "        \n",
        "        print(f\"ðŸ“Š HYPERPARAMETER TUNING ANALYSIS\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"Total trials: {len(results)}\")\n",
        "        print(f\"Successful trials: {len(valid_results)}\")\n",
        "        print(f\"Failed trials: {len(results) - len(valid_results)}\")\n",
        "        \n",
        "        if not valid_results:\n",
        "            print(\"No successful trials to analyze!\")\n",
        "            return\n",
        "        \n",
        "        # Top configurations\n",
        "        print(f\"\\nðŸ† TOP {min(top_k, len(valid_results))} CONFIGURATIONS:\")\n",
        "        print(\"-\" * 60)\n",
        "        for i, result in enumerate(valid_results[:top_k]):\n",
        "            print(f\"\\nRank {i+1}:\")\n",
        "            print(f\"  Validation Loss: {result.val_loss:.6f}\")\n",
        "            print(f\"  Parameters: {result.params:,}\")\n",
        "            print(f\"  Training Time: {result.duration_s:.1f}s\")\n",
        "            print(f\"  Epochs: {result.epochs_trained}\")\n",
        "            print(f\"  Config: {result.config}\")\n",
        "        \n",
        "        return valid_results[:top_k]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Initial data shape: (8307, 17)\n",
            "After removing missing sequences: (8304, 17)\n",
            "After removing missing probabilities: (8304, 17)\n",
            "Final dataset: 8304 samples\n",
            "Average sequence length: 600.0\n",
            "Min sequence length: 472\n",
            "Max sequence length: 600\n",
            "\n",
            "Data statistics:\n",
            "Total samples: 8304\n",
            "Target shape: (8304, 5)\n",
            "  Component 1: 0.0000 - 1.0000 (mean: 0.0878)\n",
            "  Component 2: 0.0000 - 0.9414 (mean: 0.3198)\n",
            "  Component 3: 0.0000 - 0.9999 (mean: 0.1145)\n",
            "  Component 4: 0.0000 - 0.9296 (mean: 0.2135)\n",
            "  Component 5: 0.0000 - 0.9702 (mean: 0.2644)\n",
            "\n",
            "Data splits:\n",
            "  Train: 5314, Val: 1329, Test: 1661\n",
            "Using device: mps\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "sequences, targets = load_and_prepare_data('../../data/processed/ProSeq_with_5component_analysis.csv')\n",
        "\n",
        "# Show data statistics\n",
        "print(f\"\\nData statistics:\")\n",
        "print(f\"Total samples: {len(sequences)}\")\n",
        "print(f\"Target shape: {targets.shape}\")\n",
        "for i in range(5):\n",
        "    print(f\"  Component {i+1}: {targets[:, i].min():.4f} - {targets[:, i].max():.4f} (mean: {targets[:, i].mean():.4f})\")\n",
        "\n",
        "# Split data (stratified by dominant component)\n",
        "labels = np.argmax(targets, axis=1)\n",
        "train_seq, test_seq, train_targets, test_targets = train_test_split(\n",
        "    sequences, targets, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "train_labels = np.argmax(train_targets, axis=1)\n",
        "train_seq, val_seq, train_targets, val_targets = train_test_split(\n",
        "    train_seq, train_targets, test_size=0.2, random_state=42, stratify=train_labels\n",
        ")\n",
        "\n",
        "print(f\"\\nData splits:\")\n",
        "print(f\"  Train: {len(train_seq)}, Val: {len(val_seq)}, Test: {len(test_seq)}\")\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = PromoterDataset(train_seq, train_targets)\n",
        "val_dataset = PromoterDataset(val_seq, val_targets)\n",
        "\n",
        "# Device selection\n",
        "if getattr(torch.backends, 'mps', None) is not None and torch.backends.mps.is_available():\n",
        "    device = torch.device('mps')\n",
        "else:\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Hyperparameter Optimization\n",
        "\n",
        "Execute the comprehensive hyperparameter search using multiple strategies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸŽ¯ STARTING HYPERPARAMETER OPTIMIZATION\n",
            "Random Search Trials: 20\n",
            "Random Seed: 42\n",
            "\n",
            "ðŸŽ² PHASE 1: RANDOM SEARCH\n",
            "==================================================\n",
            "ðŸ” Starting Random Search with 20 trials...\n",
            "Trial   1/20: val_loss=3.384151, params=901, time=36.9s\n",
            "Trial   2/20: val_loss=1.309438, params=4,421, time=20.1s\n",
            "Trial   3/20: val_loss=1.309438, params=901, time=15.3s\n",
            "Trial   4/20: val_loss=1.309438, params=183,269, time=33.1s\n",
            "Trial   5/20: val_loss=3.384134, params=11,189, time=27.2s\n",
            "Trial   6/20: val_loss=1.505727, params=6,197, time=32.3s\n",
            "Trial   7/20: val_loss=1.507379, params=11,189, time=13.1s\n",
            "Trial   8/20: val_loss=1.113995, params=901, time=19.6s\n",
            "Trial   9/20: val_loss=3.384212, params=324,229, time=44.6s\n",
            "Trial  10/20: val_loss=1.111563, params=11,189, time=18.0s\n",
            "   Best so far: 1.111563 with 11,189 params\n",
            "Trial  11/20: val_loss=3.384222, params=5,829, time=17.8s\n",
            "Trial  12/20: val_loss=3.384753, params=82,245, time=33.1s\n",
            "Trial  13/20: val_loss=3.384081, params=2,069, time=80.1s\n",
            "Trial  14/20: val_loss=1.110731, params=84,965, time=97.2s\n",
            "Trial  15/20: val_loss=1.115570, params=3,013, time=38.7s\n",
            "Trial  16/20: val_loss=1.114164, params=755,621, time=53.5s\n",
            "Trial  17/20: val_loss=1.112644, params=18,437, time=31.4s\n",
            "Trial  18/20: val_loss=3.384246, params=5,829, time=25.3s\n",
            "Trial  19/20: val_loss=1.309438, params=21,621, time=35.0s\n",
            "Trial  20/20: val_loss=1.309438, params=11,189, time=21.1s\n",
            "   Best so far: 1.110731 with 84,965 params\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'HyperparameterTuner' object has no attribute 'gradient_search'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m50\u001b[39m)\n\u001b[32m     15\u001b[39m random_results = tuner.random_search(num_trials=RANDOM_SEARCH_TRIALS, seed=RANDOM_SEED)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m gradient_results = \u001b[43mtuner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgradient_search\u001b[49m(num_trials=RANDOM_SEARCH_TRIALS, seed=RANDOM_SEED)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Analyze results\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mðŸ“Š ANALYSIS\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mAttributeError\u001b[39m: 'HyperparameterTuner' object has no attribute 'gradient_search'"
          ]
        }
      ],
      "source": [
        "# Initialize the tuner\n",
        "tuner = HyperparameterTuner(train_dataset, val_dataset, device)\n",
        "\n",
        "# Configuration for tuning runs\n",
        "RANDOM_SEARCH_TRIALS = 20\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "print(f\"ðŸŽ¯ STARTING HYPERPARAMETER OPTIMIZATION\")\n",
        "print(f\"Random Search Trials: {RANDOM_SEARCH_TRIALS}\")\n",
        "print(f\"Random Seed: {RANDOM_SEED}\")\n",
        "\n",
        "# Random search to explore the space\n",
        "print(f\"\\nðŸŽ² PHASE 1: RANDOM SEARCH\")\n",
        "print(\"=\"*50)\n",
        "random_results = tuner.random_search(num_trials=RANDOM_SEARCH_TRIALS, seed=RANDOM_SEED)\n",
        "gradient_results = tuner.gradient_search(num_trials=RANDOM_SEARCH_TRIALS, seed=RANDOM_SEED)\n",
        "\n",
        "# Analyze results\n",
        "print(f\"\\nðŸ“Š ANALYSIS\")\n",
        "print(\"=\"*50)\n",
        "best_configs = tuner.analyze_results(random_results + gradient_results, top_k=3)\n",
        "\n",
        "# Save results\n",
        "tuner.save_results(random_results, \"hyperparameter_tuning_results.pkl\")\n",
        "\n",
        "# Extract the best configuration\n",
        "if best_configs:\n",
        "    best_config = best_configs[0]\n",
        "    print(f\"\\nðŸŽ¯ BEST CONFIGURATION:\")\n",
        "    print(f\"   Validation Loss: {best_config.val_loss:.6f}\")\n",
        "    print(f\"   Configuration: {best_config.config}\")\n",
        "    \n",
        "    # Save best config as JSON for easy loading in main notebook\n",
        "    import json\n",
        "    with open('best_hyperparameters.json', 'w') as f:\n",
        "        json.dump(best_config.config, f, indent=2)\n",
        "    print(f\"\\nðŸ’¾ Best configuration saved to 'best_hyperparameters.json'\")\n",
        "else:\n",
        "    print(\"No successful configurations found!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
