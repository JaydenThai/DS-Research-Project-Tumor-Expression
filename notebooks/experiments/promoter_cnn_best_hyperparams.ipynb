{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Promoter CNN with Best Hyperparameters\n",
        "\n",
        "This notebook implements the Promoter CNN model using the optimal hyperparameters found through comprehensive hyperparameter tuning. It leverages the modular codebase structure and applies the best configuration from `best_hyperparameters.json`.\n",
        "\n",
        "## Key Features\n",
        "- **Optimal hyperparameters** from comprehensive tuning results\n",
        "- **One-hot encoding** of DNA sequences (A, T, G, C, N)\n",
        "- **Optimized CNN architecture** with best depth and channel configuration\n",
        "- **Advanced training setup** with optimal optimizer, scheduler, and loss function\n",
        "- **Device-agnostic training** (CUDA/MPS/CPU)\n",
        "- **Comprehensive evaluation** and visualization\n",
        "\n",
        "## Hyperparameter Configuration\n",
        "Based on `best_hyperparameters.json`:\n",
        "- **Architecture**: 4-layer depth, 64 base channels, 0.3 dropout\n",
        "- **Training**: AdamW optimizer, 0.0005 learning rate, 5e-05 weight decay\n",
        "- **Advanced**: Cosine scheduler, KL-divergence loss, gradient clipping\n",
        "- **Batch size**: 64, Max epochs: 50, Early stopping: 15 patience\n",
        "\n",
        "## Table of Contents\n",
        "1. [Environment Setup](#Environment-Setup)\n",
        "2. [Hyperparameter Loading](#Hyperparameter-Loading)\n",
        "3. [Data Loading and Preprocessing](#Data-Loading-and-Preprocessing)\n",
        "4. [Model Architecture](#Model-Architecture)\n",
        "5. [Training Configuration](#Training-Configuration)\n",
        "6. [Training Loop](#Training-Loop)\n",
        "7. [Evaluation and Results](#Evaluation-and-Results)\n",
        "8. [Visualization](#Visualization)\n",
        "9. [Model Saving](#Model-Saving)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment Setup\n",
        "\n",
        "First, let's import all necessary libraries and modules from our codebase.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: /Users/jaydenthai/Dev/University Work/2025sem2/Data Science Research Project /DS-Research-Project-Tumor-Expression\n",
            "Python path updated\n",
            "PyTorch version: 2.5.1\n",
            "CUDA available: False\n",
            "MPS available: True\n"
          ]
        }
      ],
      "source": [
        "# Standard libraries\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import json\n",
        "import time\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.optim import Adam, AdamW, SGD, RMSprop\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR, StepLR, ExponentialLR\n",
        "\n",
        "# Sklearn for metrics\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path().resolve().parent.parent\n",
        "sys.path.append(str(project_root))\n",
        "\n",
        "print(f\"Project root: {project_root}\")\n",
        "print(f\"Python path updated\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if hasattr(torch.backends, 'mps'):\n",
        "    print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
        "else:\n",
        "    print(\"MPS not available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Successfully imported all custom modules\n",
            "   üìÅ CNN Model: PromoterCNN\n",
            "   üìÅ Data Utils: PromoterDataset, load_and_prepare_data\n",
            "   üìÅ Training Utils: train_epoch, validate_epoch, evaluate_model\n",
            "   üìÅ Device Utils: create_device_manager\n",
            "   üìÅ Visualization: plot_results\n"
          ]
        }
      ],
      "source": [
        "# Import custom modules from our codebase\n",
        "try:\n",
        "    # Core model and data utilities\n",
        "    from src.models.cnn.model import PromoterCNN\n",
        "    from src.utils.data import PromoterDataset, load_and_prepare_data\n",
        "    \n",
        "    # Training and evaluation utilities\n",
        "    from src.utils.training import train_epoch, validate_epoch, evaluate_model\n",
        "    from src.utils.device import create_device_manager\n",
        "\n",
        "    from improved_hyperparameter_tuning import EnhancedPromoterCNN\n",
        "    \n",
        "    # Visualization utilities\n",
        "    from src.utils.viz import plot_results\n",
        "    \n",
        "    print(\"‚úÖ Successfully imported all custom modules\")\n",
        "    print(\"   üìÅ CNN Model: PromoterCNN\")\n",
        "    print(\"   üìÅ Data Utils: PromoterDataset, load_and_prepare_data\")\n",
        "    print(\"   üìÅ Training Utils: train_epoch, validate_epoch, evaluate_model\")\n",
        "    print(\"   üìÅ Device Utils: create_device_manager\")\n",
        "    print(\"   üìÅ Visualization: plot_results\")\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Import error: {e}\")\n",
        "    print(\"Please ensure the project structure is correct and src/ modules are available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameter Loading and Configuration\n",
        "\n",
        "Load the best hyperparameters from the JSON file and set up the experiment configuration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚öôÔ∏è  Loading best hyperparameters from JSON...\n",
            "‚úÖ Loaded best hyperparameters from: /Users/jaydenthai/Dev/University Work/2025sem2/Data Science Research Project /DS-Research-Project-Tumor-Expression/results/analysis/best_hyperparameters.json\n",
            "\n",
            "üèóÔ∏è  Architecture Configuration:\n",
            "   Depth (conv blocks): 4\n",
            "   Base channels: 64\n",
            "   Dropout: 0.3\n",
            "   Number of classes: 5\n",
            "\n",
            "üéì Training Configuration:\n",
            "   Optimizer: adamw\n",
            "   Learning rate: 0.0005\n",
            "   Weight decay: 5e-05\n",
            "   Batch size: 64\n",
            "   Max epochs: 50\n",
            "   Early stopping patience: 15\n",
            "\n",
            "üìä Advanced Configuration:\n",
            "   Loss function: kldiv\n",
            "   Scheduler: cosine\n",
            "   Gradient clipping: True\n",
            "   Max grad norm: 1.0\n",
            "   Label smoothing: 0.0\n",
            "\n",
            "üîß Experiment Setup:\n",
            "   Name: promoter_cnn_best_hyperparams\n",
            "   Sequence Length: 600\n",
            "   Random Seed: 42\n"
          ]
        }
      ],
      "source": [
        "# Load best hyperparameters from JSON file\n",
        "print(\"‚öôÔ∏è  Loading best hyperparameters from JSON...\")\n",
        "\n",
        "# Load the best hyperparameters\n",
        "best_hyperparams_path = project_root / \"results\" / \"analysis\" / \"best_hyperparameters.json\"\n",
        "\n",
        "if not best_hyperparams_path.exists():\n",
        "    raise FileNotFoundError(f\"Best hyperparameters file not found: {best_hyperparams_path}\")\n",
        "\n",
        "with open(best_hyperparams_path, 'r') as f:\n",
        "    params = json.load(f)\n",
        "\n",
        "print(f\"‚úÖ Loaded best hyperparameters from: {best_hyperparams_path}\")\n",
        "\n",
        "# Display the loaded hyperparameters\n",
        "print(f\"\\nüèóÔ∏è  Architecture Configuration:\")\n",
        "print(f\"   Depth (conv blocks): {params['depth']}\")\n",
        "print(f\"   Base channels: {params['base_channels']}\")\n",
        "print(f\"   Dropout: {params['dropout']}\")\n",
        "print(f\"   Number of classes: {params['num_classes']}\")\n",
        "\n",
        "print(f\"\\nüéì Training Configuration:\")\n",
        "print(f\"   Optimizer: {params['optimizer']}\")\n",
        "print(f\"   Learning rate: {params['learning_rate']}\")\n",
        "print(f\"   Weight decay: {params['weight_decay']}\")\n",
        "print(f\"   Batch size: {params['batch_size']}\")\n",
        "print(f\"   Max epochs: {params['max_epochs']}\")\n",
        "print(f\"   Early stopping patience: {params['early_stopping_patience']}\")\n",
        "\n",
        "print(f\"\\nüìä Advanced Configuration:\")\n",
        "print(f\"   Loss function: {params['loss_function']}\")\n",
        "print(f\"   Scheduler: {params['scheduler']}\")\n",
        "print(f\"   Gradient clipping: {params['gradient_clipping']}\")\n",
        "if params['gradient_clipping']:\n",
        "    print(f\"   Max grad norm: {params['max_grad_norm']}\")\n",
        "print(f\"   Label smoothing: {params['label_smoothing']}\")\n",
        "\n",
        "# Experiment configuration\n",
        "EXPERIMENT_NAME = \"promoter_cnn_best_hyperparams\"\n",
        "SEQUENCE_LENGTH = 600\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "print(f\"\\nüîß Experiment Setup:\")\n",
        "print(f\"   Name: {EXPERIMENT_NAME}\")\n",
        "print(f\"   Sequence Length: {SEQUENCE_LENGTH}\")\n",
        "print(f\"   Random Seed: {RANDOM_SEED}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÅ Paths Configuration:\n",
            "   Data Path: /Users/jaydenthai/Dev/University Work/2025sem2/Data Science Research Project /DS-Research-Project-Tumor-Expression/data/processed/ProSeq_with_5component_analysis.csv\n",
            "   Output Directory: /Users/jaydenthai/Dev/University Work/2025sem2/Data Science Research Project /DS-Research-Project-Tumor-Expression/results\n",
            "   Model Weights: /Users/jaydenthai/Dev/University Work/2025sem2/Data Science Research Project /DS-Research-Project-Tumor-Expression/results/model_weights\n",
            "   Plots: /Users/jaydenthai/Dev/University Work/2025sem2/Data Science Research Project /DS-Research-Project-Tumor-Expression/results/plots\n",
            "\n",
            "üñ•Ô∏è  Setting up device management...\n",
            "\n",
            "üì± Device Configuration:\n",
            "   Device: mps\n",
            "   Device Name: mps\n",
            "   DataLoader kwargs: {'num_workers': 0}\n"
          ]
        }
      ],
      "source": [
        "# Setup paths and device management\n",
        "DATA_PATH = project_root / \"data\" / \"processed\" / \"ProSeq_with_5component_analysis.csv\"\n",
        "OUTPUT_DIR = project_root / \"results\"\n",
        "MODEL_WEIGHTS_DIR = OUTPUT_DIR / \"model_weights\"\n",
        "PLOTS_DIR = OUTPUT_DIR / \"plots\"\n",
        "\n",
        "# Create output directories\n",
        "MODEL_WEIGHTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(RANDOM_SEED)\n",
        "\n",
        "print(f\"üìÅ Paths Configuration:\")\n",
        "print(f\"   Data Path: {DATA_PATH}\")\n",
        "print(f\"   Output Directory: {OUTPUT_DIR}\")\n",
        "print(f\"   Model Weights: {MODEL_WEIGHTS_DIR}\")\n",
        "print(f\"   Plots: {PLOTS_DIR}\")\n",
        "\n",
        "# Device setup using our custom device manager\n",
        "print(f\"\\nüñ•Ô∏è  Setting up device management...\")\n",
        "device_manager = create_device_manager(prefer_cuda=True, verbose=True)\n",
        "\n",
        "# Get device information\n",
        "device = device_manager.device\n",
        "device_name = device_manager.device_name\n",
        "loader_kwargs = device_manager.get_dataloader_kwargs()\n",
        "\n",
        "print(f\"\\nüì± Device Configuration:\")\n",
        "print(f\"   Device: {device}\")\n",
        "print(f\"   Device Name: {device_name}\")\n",
        "print(f\"   DataLoader kwargs: {loader_kwargs}\")\n",
        "\n",
        "# Display memory info if available\n",
        "memory_info = device_manager.get_memory_info()\n",
        "if memory_info['total'] > 0:\n",
        "    print(f\"   Memory Total: {memory_info['total']:.1f} GB\")\n",
        "    print(f\"   Memory Free: {memory_info['free']:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading and Preprocessing\n",
        "\n",
        "Load the promoter sequence data and create datasets with automatic one-hot encoding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Loading promoter sequence data...\n",
            "   Data path: /Users/jaydenthai/Dev/University Work/2025sem2/Data Science Research Project /DS-Research-Project-Tumor-Expression/data/processed/ProSeq_with_5component_analysis.csv\n",
            "\n",
            "üìà Data Overview:\n",
            "   Total sequences: 8304\n",
            "   Target shape: (8304, 5)\n",
            "   Sequence length range: 472 - 600\n",
            "\n",
            "üìä Target Statistics:\n",
            "   Component 1: mean=0.088, std=0.250\n",
            "   Component 2: mean=0.320, std=0.376\n",
            "   Component 3: mean=0.115, std=0.274\n",
            "   Component 4: mean=0.214, std=0.313\n",
            "   Component 5: mean=0.264, std=0.381\n",
            "\n",
            "üß¨ Example DNA Sequence (first 100 bp):\n",
            "   AAGCTGCACAGTCGAGCCTGCGGCTCCGCAGCCGAATAGAGCGGAAATGCCCTCTCAGGGCATCAAAGAGCAACAAGCTGCCACTGTAAGAGGGGCCCAG...\n",
            "   Length: 600 bp\n",
            "   Targets: [6.32430892e-10 1.75677897e-04 2.10593641e-01 4.88930448e-07\n",
            " 7.89230191e-01]\n",
            "\n",
            "üî¢ One-Hot Encoding Demonstration:\n",
            "   Encoded sequence shape: torch.Size([5, 600])\n",
            "   Expected: (5, 600) for Conv1d input\n",
            "   Target shape: torch.Size([5])\n",
            "   Target sum: 1.000000 (should be ~1.0)\n",
            "‚úÖ Data loading and encoding validation complete\n"
          ]
        }
      ],
      "source": [
        "# Load data using our custom data loading function\n",
        "print(\"üìä Loading promoter sequence data...\")\n",
        "print(f\"   Data path: {DATA_PATH}\")\n",
        "\n",
        "if not DATA_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Data file not found: {DATA_PATH}\")\n",
        "\n",
        "sequences, targets = load_and_prepare_data(str(DATA_PATH))\n",
        "\n",
        "print(f\"\\nüìà Data Overview:\")\n",
        "print(f\"   Total sequences: {len(sequences)}\")\n",
        "print(f\"   Target shape: {targets.shape}\")\n",
        "print(f\"   Sequence length range: {min(len(seq) for seq in sequences)} - {max(len(seq) for seq in sequences)}\")\n",
        "\n",
        "# Display data statistics\n",
        "print(f\"\\nüìä Target Statistics:\")\n",
        "for i in range(5):\n",
        "    print(f\"   Component {i+1}: mean={targets[:, i].mean():.3f}, std={targets[:, i].std():.3f}\")\n",
        "\n",
        "# Show example sequence\n",
        "print(f\"\\nüß¨ Example DNA Sequence (first 100 bp):\")\n",
        "print(f\"   {sequences[0][:100]}...\")\n",
        "print(f\"   Length: {len(sequences[0])} bp\")\n",
        "print(f\"   Targets: {targets[0]}\")\n",
        "\n",
        "# Demonstrate one-hot encoding\n",
        "print(f\"\\nüî¢ One-Hot Encoding Demonstration:\")\n",
        "sample_dataset = PromoterDataset([sequences[0]], targets[:1], max_length=SEQUENCE_LENGTH)\n",
        "sample_batch = sample_dataset[0]\n",
        "\n",
        "encoded_sequence = sample_batch[\"sequence\"]\n",
        "target_probs = sample_batch[\"target\"]\n",
        "\n",
        "print(f\"   Encoded sequence shape: {encoded_sequence.shape}\")\n",
        "print(f\"   Expected: (5, {SEQUENCE_LENGTH}) for Conv1d input\")\n",
        "print(f\"   Target shape: {target_probs.shape}\")\n",
        "print(f\"   Target sum: {target_probs.sum():.6f} (should be ~1.0)\")\n",
        "\n",
        "print(\"‚úÖ Data loading and encoding validation complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÄ Creating train/validation/test splits...\n",
            "   Total samples: 8304\n",
            "   Train: 5812 (70.0%)\n",
            "   Validation: 1245 (15.0%)\n",
            "   Test: 1247 (15.0%)\n",
            "\n",
            "‚úÖ Dataset splits created:\n",
            "   Train dataset: 5812 samples\n",
            "   Validation dataset: 1245 samples\n",
            "   Test dataset: 1247 samples\n"
          ]
        }
      ],
      "source": [
        "# Create train/validation/test splits\n",
        "print(\"üîÄ Creating train/validation/test splits...\")\n",
        "\n",
        "# Split ratios\n",
        "train_ratio = 0.7\n",
        "val_ratio = 0.15\n",
        "test_ratio = 0.15\n",
        "\n",
        "# Calculate split sizes\n",
        "total_size = len(sequences)\n",
        "train_size = int(train_ratio * total_size)\n",
        "val_size = int(val_ratio * total_size)\n",
        "test_size = total_size - train_size - val_size\n",
        "\n",
        "print(f\"   Total samples: {total_size}\")\n",
        "print(f\"   Train: {train_size} ({train_ratio:.1%})\")\n",
        "print(f\"   Validation: {val_size} ({val_ratio:.1%})\")\n",
        "print(f\"   Test: {test_size} ({test_ratio:.1%})\")\n",
        "\n",
        "# Create full dataset first\n",
        "full_dataset = PromoterDataset(sequences, targets, max_length=SEQUENCE_LENGTH)\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, temp_dataset = random_split(\n",
        "    full_dataset, \n",
        "    [train_size, val_size + test_size],\n",
        "    generator=torch.Generator().manual_seed(RANDOM_SEED)\n",
        ")\n",
        "\n",
        "val_dataset, test_dataset = random_split(\n",
        "    temp_dataset, \n",
        "    [val_size, test_size],\n",
        "    generator=torch.Generator().manual_seed(RANDOM_SEED)\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Dataset splits created:\")\n",
        "print(f\"   Train dataset: {len(train_dataset)} samples\")\n",
        "print(f\"   Validation dataset: {len(val_dataset)} samples\")\n",
        "print(f\"   Test dataset: {len(test_dataset)} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Architecture\n",
        "\n",
        "Create the PromoterCNN model using the optimal hyperparameters from the JSON configuration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Creating PromoterCNN model with optimal hyperparameters...\n",
            "‚úÖ Model created with optimal configuration:\n",
            "   Architecture: 4 blocks, 64 base channels\n",
            "   Dropout: 0.3\n",
            "   Sequence length: 600\n",
            "   Output classes: 5\n",
            "\n",
            "üìã Model Summary:\n",
            "   Total parameters: 32,645\n",
            "   Trainable parameters: 32,645\n",
            "   Model device: mps:0\n",
            "\n",
            "üèóÔ∏è  Model Architecture:\n",
            "PromoterCNN(\n",
            "  (sequence_conv): Sequential(\n",
            "    (0): Conv1d(5, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Dropout(p=0.3, inplace=False)\n",
            "    (4): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
            "    (5): ReLU()\n",
            "    (6): Dropout(p=0.3, inplace=False)\n",
            "    (7): AdaptiveAvgPool1d(output_size=1)\n",
            "  )\n",
            "  (classifier): Linear(in_features=64, out_features=5, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Create PromoterCNN model using best hyperparameters\n",
        "print(\"üöÄ Creating PromoterCNN model with optimal hyperparameters...\")\n",
        "\n",
        "# Create model using the loaded parameters\n",
        "model = PromoterCNN(\n",
        "    sequence_length=SEQUENCE_LENGTH, \n",
        "    num_blocks=params['depth'], \n",
        "    base_channels=params['base_channels'], \n",
        "    dropout=params['dropout'], \n",
        "    num_classes=params['num_classes']\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Model created with optimal configuration:\")\n",
        "print(f\"   Architecture: {params['depth']} blocks, {params['base_channels']} base channels\")\n",
        "print(f\"   Dropout: {params['dropout']}\")\n",
        "print(f\"   Sequence length: {SEQUENCE_LENGTH}\")\n",
        "print(f\"   Output classes: {params['num_classes']}\")\n",
        "\n",
        "# Move model to device\n",
        "model = device_manager.create_model_wrapper(model)\n",
        "\n",
        "# Display model summary\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\nüìã Model Summary:\")\n",
        "print(f\"   Total parameters: {total_params:,}\")\n",
        "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"   Model device: {next(model.parameters()).device}\")\n",
        "\n",
        "# Print model architecture\n",
        "print(f\"\\nüèóÔ∏è  Model Architecture:\")\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Creating data loaders with optimal batch size...\n",
            "‚úÖ Data loaders created with optimal configuration:\n",
            "   Train loader: 90 batches\n",
            "   Validation loader: 19 batches\n",
            "   Test loader: 19 batches\n",
            "   Batch size: 64 (optimized)\n",
            "\n",
            "üß™ Testing data loading and model forward pass...\n",
            "   Batch sequences shape: torch.Size([64, 5, 600])\n",
            "   Batch targets shape: torch.Size([64, 5])\n",
            "   Model output shape: torch.Size([64, 5])\n",
            "   Output device: mps:0\n",
            "‚úÖ Data loading and model forward pass successful!\n"
          ]
        }
      ],
      "source": [
        "# Create data loaders with optimal batch size\n",
        "print(\"üîÑ Creating data loaders with optimal batch size...\")\n",
        "\n",
        "# Use the optimal batch size from hyperparameters\n",
        "dataloader_kwargs = device_manager.get_dataloader_kwargs(\n",
        "    batch_size=params['batch_size'],\n",
        "    shuffle=True,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, **dataloader_kwargs)\n",
        "\n",
        "# Validation and test loaders don't need shuffling\n",
        "val_kwargs = dataloader_kwargs.copy()\n",
        "val_kwargs['shuffle'] = False\n",
        "val_loader = DataLoader(val_dataset, **val_kwargs)\n",
        "test_loader = DataLoader(test_dataset, **val_kwargs)\n",
        "\n",
        "print(f\"‚úÖ Data loaders created with optimal configuration:\")\n",
        "print(f\"   Train loader: {len(train_loader)} batches\")\n",
        "print(f\"   Validation loader: {len(val_loader)} batches\")\n",
        "print(f\"   Test loader: {len(test_loader)} batches\")\n",
        "print(f\"   Batch size: {params['batch_size']} (optimized)\")\n",
        "\n",
        "# Test a batch to ensure everything works\n",
        "print(f\"\\nüß™ Testing data loading and model forward pass...\")\n",
        "sample_batch = next(iter(train_loader))\n",
        "sample_sequences = sample_batch[\"sequence\"]\n",
        "sample_targets = sample_batch[\"target\"]\n",
        "\n",
        "print(f\"   Batch sequences shape: {sample_sequences.shape}\")\n",
        "print(f\"   Batch targets shape: {sample_targets.shape}\")\n",
        "\n",
        "# Test model forward pass\n",
        "with torch.no_grad():\n",
        "    sample_sequences = device_manager.to_device(sample_sequences)\n",
        "    sample_output = model(sample_sequences)\n",
        "    print(f\"   Model output shape: {sample_output.shape}\")\n",
        "    print(f\"   Output device: {sample_output.device}\")\n",
        "\n",
        "print(\"‚úÖ Data loading and model forward pass successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Configuration\n",
        "\n",
        "Set up optimizer, loss function, and scheduler using the optimal hyperparameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéØ Setting up optimizer: adamw\n",
            "‚úÖ Optimizer created: AdamW\n",
            "   Learning rate: 0.0005\n",
            "   Weight decay: 5e-05\n",
            "\n",
            "üéØ Setting up loss function: kldiv\n",
            "‚úÖ Loss function created: KLDivLoss\n",
            "\n",
            "üìä Setting up scheduler: cosine\n",
            "‚úÖ Scheduler created: CosineAnnealingLR\n",
            "\n",
            "üöÄ Optimal training setup complete!\n",
            "   Model: PromoterCNN\n",
            "   Optimizer: AdamW\n",
            "   Loss function: KLDivLoss\n",
            "   Scheduler: CosineAnnealingLR\n",
            "   Gradient clipping: True\n",
            "   Max grad norm: 1.0\n",
            "   Device: mps\n",
            "\n",
            "üìã Complete Optimal Configuration Summary:\n",
            "   üèóÔ∏è  Architecture: 4 blocks, 64 channels, 0.3 dropout\n",
            "   üéØ Training: adamw, lr=0.0005, wd=5e-05\n",
            "   üìä Schedule: cosine, batch_size=64, epochs=50\n",
            "   üîß Advanced: patience=15, clipping=True\n"
          ]
        }
      ],
      "source": [
        "# Set up optimizer based on optimal hyperparameters\n",
        "print(f\"üéØ Setting up optimizer: {params['optimizer']}\")\n",
        "\n",
        "# Create optimizer with optimal parameters\n",
        "optimizer_params = {\n",
        "    'lr': params['learning_rate'],\n",
        "    'weight_decay': params['weight_decay']\n",
        "}\n",
        "\n",
        "if params['optimizer'].lower() == 'adamw':\n",
        "    optimizer = AdamW(model.parameters(), **optimizer_params)\n",
        "elif params['optimizer'].lower() == 'adam':\n",
        "    optimizer = Adam(model.parameters(), **optimizer_params)\n",
        "elif params['optimizer'].lower() == 'sgd':\n",
        "    optimizer = SGD(model.parameters(), **optimizer_params, momentum=0.9)\n",
        "elif params['optimizer'].lower() == 'rmsprop':\n",
        "    optimizer = RMSprop(model.parameters(), **optimizer_params)\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Unknown optimizer {params['optimizer']}, using AdamW\")\n",
        "    optimizer = AdamW(model.parameters(), **optimizer_params)\n",
        "\n",
        "print(f\"‚úÖ Optimizer created: {type(optimizer).__name__}\")\n",
        "print(f\"   Learning rate: {params['learning_rate']}\")\n",
        "print(f\"   Weight decay: {params['weight_decay']}\")\n",
        "\n",
        "# Set up loss function\n",
        "print(f\"\\nüéØ Setting up loss function: {params['loss_function']}\")\n",
        "label_smoothing = params.get('label_smoothing', 0.0)\n",
        "\n",
        "if params['loss_function'].lower() == 'kldiv':\n",
        "    criterion = nn.KLDivLoss(reduction='batchmean')\n",
        "elif params['loss_function'].lower() == 'mse':\n",
        "    criterion = nn.MSELoss()\n",
        "elif params['loss_function'].lower() == 'crossentropy':\n",
        "    if label_smoothing > 0:\n",
        "        criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
        "        print(f\"   Label smoothing: {label_smoothing}\")\n",
        "    else:\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Unknown loss function {params['loss_function']}, using KLDivLoss\")\n",
        "    criterion = nn.KLDivLoss(reduction='batchmean')\n",
        "\n",
        "print(f\"‚úÖ Loss function created: {type(criterion).__name__}\")\n",
        "if label_smoothing > 0:\n",
        "    print(f\"   Label smoothing: {label_smoothing}\")\n",
        "\n",
        "# Set up learning rate scheduler\n",
        "print(f\"\\nüìä Setting up scheduler: {params['scheduler']}\")\n",
        "\n",
        "if params['scheduler'].lower() == 'cosine':\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=params['max_epochs'])\n",
        "elif params['scheduler'].lower() == 'plateau':\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n",
        "elif params['scheduler'].lower() == 'step':\n",
        "    scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "elif params['scheduler'].lower() == 'exponential':\n",
        "    scheduler = ExponentialLR(optimizer, gamma=0.95)\n",
        "elif params['scheduler'].lower() == 'none':\n",
        "    scheduler = None\n",
        "    print(\"   No scheduler selected\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Unknown scheduler {params['scheduler']}, using CosineAnnealingLR\")\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=params['max_epochs'])\n",
        "\n",
        "if scheduler:\n",
        "    print(f\"‚úÖ Scheduler created: {type(scheduler).__name__}\")\n",
        "\n",
        "# Set up gradient clipping if specified\n",
        "gradient_clipping = params.get('gradient_clipping', False)\n",
        "max_grad_norm = params.get('max_grad_norm', 1.0)\n",
        "\n",
        "print(f\"\\nüöÄ Optimal training setup complete!\")\n",
        "print(f\"   Model: {type(model).__name__}\")\n",
        "print(f\"   Optimizer: {type(optimizer).__name__}\")\n",
        "print(f\"   Loss function: {type(criterion).__name__}\")\n",
        "print(f\"   Scheduler: {type(scheduler).__name__ if scheduler else 'None'}\")\n",
        "print(f\"   Gradient clipping: {gradient_clipping}\")\n",
        "if gradient_clipping:\n",
        "    print(f\"   Max grad norm: {max_grad_norm}\")\n",
        "print(f\"   Device: {device}\")\n",
        "\n",
        "print(f\"\\nüìã Complete Optimal Configuration Summary:\")\n",
        "print(f\"   üèóÔ∏è  Architecture: {params['depth']} blocks, {params['base_channels']} channels, {params['dropout']} dropout\")\n",
        "print(f\"   üéØ Training: {params['optimizer']}, lr={params['learning_rate']}, wd={params['weight_decay']}\")\n",
        "print(f\"   üìä Schedule: {params['scheduler']}, batch_size={params['batch_size']}, epochs={params['max_epochs']}\")\n",
        "print(f\"   üîß Advanced: patience={params['early_stopping_patience']}, clipping={gradient_clipping}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
